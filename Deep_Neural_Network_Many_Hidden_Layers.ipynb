{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import sys\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (30pts) Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot Encoding the categorical output values to binary by adding 1's for that index and 0's otherwise\n",
    "def oneHotEncode(y):\n",
    "    enc = pd.get_dummies(y['y'])\n",
    "    return np.matrix(enc)\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while forward propagation\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do squishing on the linear function\n",
    "def sigmoid(z):\n",
    "    return 1./(1+np.exp(-z))\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while backward propagation to get gradients\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do undo the squishing on the linear function\n",
    "def sigmoid_prime(z):\n",
    "    inv = (np.exp(-z))/(np.power((1+np.exp(-z)),2))\n",
    "    return inv\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z,0)\n",
    "    \n",
    "def relu_prime(z):\n",
    "    return np.where(z < 0, 0.0, 1.0)\n",
    "\n",
    "# Get the loss of for the training example\n",
    "def get_cost(Y, Yhat):\n",
    "    m= Y.shape[1]\n",
    "    loss = np.multiply(np.log(Yhat),Y) + np.multiply((1.-Y), np.log(1. - Yhat))\n",
    "    loss = np.sum(loss)\n",
    "    cost = -1./m * np.sum(loss)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost\n",
    "\n",
    "# Forward propagation to calculate yHat by applying activation function twice\n",
    "def forward_propagate(cache, layerCount, activationFuncs):\n",
    "    for l in range(1, layerCount+1):\n",
    "        Zl =  'Z' + str(l)\n",
    "        Al =  'A' + str(l)\n",
    "        Wl =  'W' + str(l)\n",
    "        Al1 = 'A' + str(l-1)\n",
    "        bl =  'b' + str(l)\n",
    "        cache[Zl] = np.dot(cache[Wl], cache[Al1]) + cache[bl]\n",
    "        activationFunc = activationFuncs[l] + '(cache[Zl])'\n",
    "        cache[Al] = eval(activationFunc)\n",
    "    return cache\n",
    "\n",
    "# Backward Propagation function to calculate the gradients\n",
    "def back_propagate(cache, layerCount, deactivationFuncs):\n",
    "    gradients = {}\n",
    "    m = cache['A0'].shape[1]\n",
    "    for l in xrange(layerCount, 0, -1):\n",
    "        if(l==layerCount): # is last layer\n",
    "            dZl = cache['A' + str(l)] - cache['A' + str(l+1)]    \n",
    "        else:\n",
    "            Zl = 'Z'+ str(l)\n",
    "            dZl1 = 'dZ'+str(l+1)\n",
    "            deacFunc = deactivationFuncs[l] + '(cache[Zl])'\n",
    "            actv_prime = eval(deacFunc)\n",
    "            term1 = cache['W'+str(l+1)].T\n",
    "            term2 = gradients[dZl1]\n",
    "            product = np.dot( term1, term2)\n",
    "            dZl = np.multiply(product, actv_prime) \n",
    "        gradients['dZ' + str(l)] = dZl\n",
    "        Al1 = cache['A' + str(l-1)]\n",
    "#         print('--dZ'+str(l))\n",
    "#         print(dZl.shape)\n",
    "#         print('A'+str(l-1))\n",
    "#         print( Al1.shape)\n",
    "        dWl = (1./m) * np.dot(dZl, Al1.T)\n",
    "        dbl = (1./m) * np.sum(dZl, axis=1)\n",
    "#         print('dW' +str(l))\n",
    "#         print(dWl.shape)\n",
    "#         print('db' +str(l))\n",
    "#         print(dbl.shape)\n",
    "        gradients['dW' + str(l)] = dWl\n",
    "        gradients['db' + str(l)] = dbl\n",
    "    return gradients  \n",
    "\n",
    "\n",
    "def gradientDescent(X, Y, YOrg, alpha, iters, hiddenLayers, layerSizes, activationFuncs, deactivationFuncs):  \n",
    "    \n",
    "    cache = initialize_parameters(hiddenLayers, layerSizes,activationFuncs )\n",
    "    cache['A0'] = X\n",
    "    ykey = 'A' + str(hiddenLayers+1)\n",
    "    cache[ykey] = Y\n",
    "    old_cost = sys.maxsize\n",
    "    new_cost = sys.maxsize\n",
    "    cost_history = []\n",
    "    for i in range(iters):\n",
    "        # Call Forward propagation to calculate yHat\n",
    "        cache = forward_propagate(cache, hiddenLayers, activationFuncs)\n",
    "        old_cost = new_cost\n",
    "        Afinal = cache['A'+str(hiddenLayers)]\n",
    "        new_cost = get_cost(Y, cache['A'+str(hiddenLayers)])\n",
    "        gradients = back_propagate(cache, hiddenLayers, deactivationFuncs)\n",
    "        for l in range(1,hiddenLayers+1):\n",
    "            Wl = cache['W'+str(l)]\n",
    "            Wl = Wl - alpha * gradients['dW'+str(l)]\n",
    "            cache['W'+str(l)] = Wl\n",
    "            bl = cache['b'+str(l)]\n",
    "            bl = bl - alpha * gradients['db'+str(l)]\n",
    "            cache['b'+str(l)] = bl\n",
    "        if(abs(old_cost - new_cost) < 0.00000001):\n",
    "            print(\"breaking\" + str(old_cost) + str(new_cost))\n",
    "            break;\n",
    "        if(i%500 ==0):\n",
    "            print (\"cost : \" + str(new_cost) + \" Old cost : \" + str(old_cost) + \" Iteration: \" + str(i))\n",
    "        cost_history.append(new_cost)\n",
    "    Afinal = softmax(Afinal) # Apply softmax to get the actual labels\n",
    "    cache['A'+str(hiddenLayers)] = Afinal\n",
    "    accuracy = get_accuracy(YOrg, Afinal)\n",
    "    return cache, cost_history, new_cost, accuracy\n",
    "\n",
    "# Softmax activation function to get the probablity of the classes\n",
    "def softmax(z):\n",
    "    softMax = (np.exp(z) / np.sum(np.exp(z),axis=0))\n",
    "    softMax = np.matrix(np.argmax(softMax,axis=0)).T\n",
    "    return softMax\n",
    "\n",
    "def plotCostHistory(cost_history, alpha, i):\n",
    "     line = plt.plot(cost_history, label=alpha)\n",
    "     plt.ylabel('Cost');\n",
    "     plt.xlabel('Iterations');\n",
    "     plt.title('Cost Progression with Iterations for different learning rates')\n",
    "     plt.legend()\n",
    "        \n",
    "def get_accuracy(Y, Ypred):\n",
    "    Y = np.matrix(Y)\n",
    "    numcorrect = 0\n",
    "    for (x,y) in zip(Ypred,Y):\n",
    "        if(x[0]==y[0]):\n",
    "            numcorrect+=1\n",
    "    accuracy=numcorrect*100.0/len(Y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the training data\n",
    "data_train = pd.read_csv('exam1_train.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "# Remove the un-necessary column\n",
    "df_train = data_train.drop('Unnamed: 0',axis=1)\n",
    "m = df_train.shape[0]\n",
    "\n",
    "y_train = pd.DataFrame(df_train['y'])\n",
    "X_train = df_train.drop(['y'], axis=1)\n",
    "\n",
    "X_train_mat = np.matrix(X_train).T\n",
    "y_train_mat = oneHotEncode(y_train).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y\n",
      "202  4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1071f1950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the test data\n",
    "data_test = pd.read_csv('exam1_test.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "# Remove the un-necessary column\n",
    "df_test = data_test.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "y_test = pd.DataFrame(df_test['y'])\n",
    "X_test = df_test.drop(['y'], axis=1)\n",
    "\n",
    "X_test_mat = np.matrix(X_test).T\n",
    "y_test_mat = oneHotEncode(y_test).T\n",
    "\n",
    "# Plot the selected pixel\n",
    "num = 202\n",
    "pixels = np.array(X_test[num:num+1], dtype='uint8')\n",
    "print(y_test[num:num+1])\n",
    "pixels = pixels.reshape((20, 20)).T\n",
    "plt.imshow(pixels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (10pts) Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights // TODO - change the initialization method\n",
    "\n",
    "np.random.seed(1) # Setting random seed to 1\n",
    "def initialize_parameters(hiddenLayers, layerSizes, activationFuncs):\n",
    "    cache = {}\n",
    "    for l in range(1,hiddenLayers+1):\n",
    "        #print(l)\n",
    "        Wl = 'W' + str(l)\n",
    "        bl = 'b' + str(l)\n",
    "        #print(layerSizes[l])\n",
    "        #print(layerSizes[l-1])\n",
    "        print('activation is ' + str(activationFuncs[l]) + ' for layer ' + str(l))\n",
    "        if(activationFuncs[l]=='sigmoid'):\n",
    "            print('activation is sigmoid for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * np.sqrt(2./layerSizes[l-1])\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        elif(activationFuncs[l]=='relu'):\n",
    "            print('activation is relu for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * (2./np.sqrt(layerSizes[l-1]))\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        else:\n",
    "            print('activation is random for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * 0.01 ## Random\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        l=l+1\n",
    "    #print(cache)\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (30pts) Deep Neural Network model with more than 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of hidden layers including output layer are - 4\n"
     ]
    }
   ],
   "source": [
    "# Defining Hyperparameters\n",
    "hiddenLayers = 4\n",
    "layerSizes = [400, 100, 100, 100, 10] # As specified in assignment requirements, can have as many hidden layers\n",
    "print(\" Number of hidden layers including output layer are - \" + str(hiddenLayers))\n",
    "activationFuncs = ['', 'relu', 'relu', 'relu', 'sigmoid']\n",
    "deactivationFuncs = ['', 'relu_prime', 'relu_prime', 'relu_prime', 'sigmoid_prime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cache = initialize_parameters(hiddenLayers, layerSizes, activationFuncs)\n",
    "# print(cache['W1'].shape)\n",
    "# # print(cache['W2'].shape)\n",
    "# # print(cache['W3'].shape)\n",
    "# # print(cache['b1'].shape)\n",
    "# # print(cache['b2'].shape)\n",
    "# # print(cache['b3'].shape)\n",
    "# cache['A0'] = X_train_mat\n",
    "# ykey = 'A' + str(len(layerSizes))\n",
    "# cache[ykey] = y_train_mat\n",
    "# cache = forward_propagate(cache, hiddenLayers, ['', 'apply_sigmoid', 'apply_sigmoid','apply_sigmoid'])\n",
    "# # print(cache['A1'])\n",
    "# # print(cache['A2'])\n",
    "# # print(cache['A3'])\n",
    "# # print(cache['Z2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# gradients = back_propagate(cache, hiddenLayers, deactivationFuncs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. (10pts) Predictions\n",
    "\n",
    "### 6. (20pts) Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimization function to check cost propagartion for different learning rates\n",
    "def optimize():\n",
    "    alpha = [1, 0.9, 0.8, 0.5, 0.3, 0.1, 0.05]\n",
    "    i=0\n",
    "    scores = pd.DataFrame(columns=['alpha','cache','accuracy'])\n",
    "    print('*****************Training Data*********************')\n",
    "    for a in alpha:\n",
    "        cache, cost_history, new_cost, acc = gradientDescent(X_train_mat, y_train_mat, \n",
    "                                                y_train, a, 1500, hiddenLayers, layerSizes, activationFuncs, deactivationFuncs)\n",
    "        scores.loc[i] = pd.Series({'alpha':a, 'cache': cache, 'accuracy':acc})\n",
    "        print(\"Cost with \" + \"Alpha \" + str(a) + \" is \" + str(new_cost) + \" & \" + \"Accuracy is \" + str(acc) + \" %\")\n",
    "        plotCostHistory(cost_history, a, i)\n",
    "        i+=1\n",
    "    least_cost_comb = scores['accuracy'].idxmax()\n",
    "    alph = scores.iloc[[least_cost_comb]]['alpha'][least_cost_comb]\n",
    "    cache = scores.iloc[[least_cost_comb]]['cache'][least_cost_comb]\n",
    "    plt.show()\n",
    "    return cache, alph,scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Training Data*********************\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 3\n",
      "activation is relu for layer 3\n",
      "activation is sigmoid for layer 4\n",
      "activation is sigmoid for layer 4\n",
      "cost : 8.45176548356 Old cost : 9223372036854775807 Iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meenakshiparyani/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/meenakshiparyani/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/meenakshiparyani/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breaking3.250346555173.25034654627\n",
      "Cost with Alpha 1 is 3.25034654627 & Accuracy is 10.5142857143 %\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 3\n",
      "activation is relu for layer 3\n",
      "activation is sigmoid for layer 4\n",
      "activation is sigmoid for layer 4\n",
      "cost : 12.0796173937 Old cost : 9223372036854775807 Iteration: 0\n",
      "breaking3.250346560573.25034655172\n",
      "Cost with Alpha 0.9 is 3.25034655172 & Accuracy is 10.5142857143 %\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 3\n",
      "activation is relu for layer 3\n",
      "activation is sigmoid for layer 4\n",
      "activation is sigmoid for layer 4\n",
      "cost : 12.6321376562 Old cost : 9223372036854775807 Iteration: 0\n",
      "breaking3.250346534443.25034652671\n",
      "Cost with Alpha 0.8 is 3.25034652671 & Accuracy is 10.5142857143 %\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 3\n",
      "activation is relu for layer 3\n",
      "activation is sigmoid for layer 4\n",
      "activation is sigmoid for layer 4\n",
      "cost : 9.51217053668 Old cost : 9223372036854775807 Iteration: 0\n",
      "cost : 0.283193356948 Old cost : 0.285960440682 Iteration: 500\n",
      "cost : 0.297439338896 Old cost : 0.278977481259 Iteration: 1000\n",
      "Cost with Alpha 0.5 is 0.0839275988191 & Accuracy is 98.8285714286 %\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 3\n",
      "activation is relu for layer 3\n",
      "activation is sigmoid for layer 4\n",
      "activation is sigmoid for layer 4\n",
      "cost : 9.46027123118 Old cost : 9223372036854775807 Iteration: 0\n",
      "cost : 0.0375283958115 Old cost : 0.0377079539112 Iteration: 500\n",
      "cost : 0.00772708830065 Old cost : 0.00774713439933 Iteration: 1000\n",
      "Cost with Alpha 0.3 is nan & Accuracy is 100.0 %\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 3\n",
      "activation is relu for layer 3\n",
      "activation is sigmoid for layer 4\n",
      "activation is sigmoid for layer 4\n",
      "cost : 8.97084632881 Old cost : 9223372036854775807 Iteration: 0\n",
      "cost : 0.126583080216 Old cost : 0.126986909688 Iteration: 500\n",
      "cost : 0.0336743698554 Old cost : 0.0337476850616 Iteration: 1000\n",
      "Cost with Alpha 0.1 is 0.0140724956717 & Accuracy is 100.0 %\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 3\n",
      "activation is relu for layer 3\n",
      "activation is sigmoid for layer 4\n",
      "activation is sigmoid for layer 4\n",
      "cost : 11.3042385792 Old cost : 9223372036854775807 Iteration: 0\n",
      "cost : 0.839910597946 Old cost : 0.84097660486 Iteration: 500\n",
      "cost : 0.548270552729 Old cost : 0.548607298829 Iteration: 1000\n",
      "Cost with Alpha 0.01 is 0.422727569012 & Accuracy is 94.6 %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEWCAYAAACe8xtsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5CEiIgVT8hARkYIpeYiISMH+P7HbbdJuMp+VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1089b94d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha is 0.3\n"
     ]
    }
   ],
   "source": [
    "# Get the best combination of weights and alpha\n",
    "cache, alph,scores = optimize()\n",
    "print('Best alpha is ' + str(alph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Test Data************************\n",
      "Test Data Optimized accuracy is 8.8 for alpha 1\n",
      "*****************Test Data************************\n",
      "Test Data Optimized accuracy is 8.8 for alpha 0.9\n",
      "*****************Test Data************************\n",
      "Test Data Optimized accuracy is 8.8 for alpha 0.8\n",
      "*****************Test Data************************\n",
      "Test Data Optimized accuracy is 89.2 for alpha 0.5\n",
      "*****************Test Data************************\n",
      "Test Data Optimized accuracy is 92.4 for alpha 0.3\n",
      "*****************Test Data************************\n",
      "Test Data Optimized accuracy is 92.7333333333 for alpha 0.1\n",
      "*****************Test Data************************\n",
      "Test Data Optimized accuracy is 90.1333333333 for alpha 0.01\n"
     ]
    }
   ],
   "source": [
    "for index, row in scores.iterrows():\n",
    "    a = row['alpha']\n",
    "    cache = row['cache']\n",
    "    cache['A0'] = X_test_mat\n",
    "    ykey = 'A' + str(hiddenLayers+1)\n",
    "    cache2 = forward_propagate(cache, hiddenLayers, activationFuncs)\n",
    "    Afinal = softmax(cache2['A'+str(hiddenLayers)])\n",
    "    acc = get_accuracy(y_test, Afinal)\n",
    "    print('*****************Test Data************************')\n",
    "    print('Test Data Optimized accuracy is ' + str(acc) + ' for alpha ' + str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
