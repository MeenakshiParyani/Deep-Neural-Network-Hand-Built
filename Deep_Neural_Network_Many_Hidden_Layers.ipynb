{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import sys\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (30pts) Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot Encoding the categorical output values to binary by adding 1's for that index and 0's otherwise\n",
    "def oneHotEncode(y):\n",
    "    enc = pd.get_dummies(y['y'])\n",
    "    return np.matrix(enc)\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while forward propagation\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do squishing on the linear function\n",
    "def sigmoid(z):\n",
    "    return 1./(1+np.exp(-z))\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while backward propagation to get gradients\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do undo the squishing on the linear function\n",
    "def sigmoid_prime(z):\n",
    "    inv = (np.exp(-z))/(np.power((1+np.exp(-z)),2))\n",
    "    return inv\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z,0)\n",
    "    \n",
    "def relu_prime(z):\n",
    "    return np.where(z < 0, 0.0, 1.0)\n",
    "\n",
    "# Get the loss of for the training example\n",
    "def get_cost(Y, Yhat, lamda, Wl):\n",
    "    m= Y.shape[1]\n",
    "    loss = np.multiply(np.log(Yhat),Y) + np.multiply((1.-Y), np.log(1. - Yhat))\n",
    "    loss = np.sum(loss)\n",
    "    cost = -1./m * np.sum(loss) + ((lamda/(2*m)) * np.sum(np.square(Wl)))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost\n",
    "\n",
    "# Forward propagation to calculate yHat by applying activation function twice\n",
    "def forward_propagate(cache, layerCount, activationFuncs):\n",
    "    for l in range(1, layerCount+1):\n",
    "        Zl =  'Z' + str(l)\n",
    "        Al =  'A' + str(l)\n",
    "        Wl =  'W' + str(l)\n",
    "        Al1 = 'A' + str(l-1)\n",
    "        bl =  'b' + str(l)\n",
    "        cache[Zl] = np.dot(cache[Wl], cache[Al1]) + cache[bl]\n",
    "        activationFunc = activationFuncs[l] + '(cache[Zl])'\n",
    "        cache[Al] = eval(activationFunc)\n",
    "    return cache\n",
    "\n",
    "# Backward Propagation function to calculate the gradients\n",
    "def back_propagate(cache, layerCount, deactivationFuncs):\n",
    "    gradients = {}\n",
    "    m = cache['A0'].shape[1]\n",
    "    for l in xrange(layerCount, 0, -1):\n",
    "        if(l==layerCount): # is last layer\n",
    "            dZl = cache['A' + str(l)] - cache['A' + str(l+1)]    \n",
    "        else:\n",
    "            Zl = 'Z'+ str(l)\n",
    "            dZl1 = 'dZ'+str(l+1)\n",
    "            deacFunc = deactivationFuncs[l] + '(cache[Zl])'\n",
    "            actv_prime = eval(deacFunc)\n",
    "            term1 = cache['W'+str(l+1)].T\n",
    "            term2 = gradients[dZl1]\n",
    "            product = np.dot( term1, term2)\n",
    "            dZl = np.multiply(product, actv_prime) \n",
    "        gradients['dZ' + str(l)] = dZl\n",
    "        Al1 = cache['A' + str(l-1)]\n",
    "#         print('--dZ'+str(l))\n",
    "#         print(dZl.shape)\n",
    "#         print('A'+str(l-1))\n",
    "#         print( Al1.shape)\n",
    "        dWl = (1./m) * np.dot(dZl, Al1.T)\n",
    "        dbl = (1./m) * np.sum(dZl, axis=1)\n",
    "#         print('dW' +str(l))\n",
    "#         print(dWl.shape)\n",
    "#         print('db' +str(l))\n",
    "#         print(dbl.shape)\n",
    "        gradients['dW' + str(l)] = dWl\n",
    "        gradients['db' + str(l)] = dbl\n",
    "    return gradients  \n",
    "\n",
    "\n",
    "def gradientDescent(X, Y, YOrg, alpha, iters, hiddenLayers, layerSizes, activationFuncs, deactivationFuncs, lamda, Xtest, Ytest):  \n",
    "    \n",
    "    cache = initialize_parameters(hiddenLayers, layerSizes,activationFuncs )\n",
    "    cache['A0'] = X\n",
    "    ykey = 'A' + str(hiddenLayers+1)\n",
    "    cache[ykey] = Y\n",
    "    old_cost = sys.maxsize\n",
    "    new_cost = sys.maxsize\n",
    "    cost_history = []\n",
    "    maxTestAcc = -sys.maxsize -1\n",
    "    for i in range(iters):\n",
    "        # Call Forward propagation to calculate yHat\n",
    "        cache = forward_propagate(cache, hiddenLayers, activationFuncs)\n",
    "        old_cost = new_cost\n",
    "        Afinal = cache['A'+str(hiddenLayers)]\n",
    "        new_cost = get_cost(Y, cache['A'+str(hiddenLayers)], lamda, cache['W'+str(hiddenLayers)]) # Regularization\n",
    "        gradients = back_propagate(cache, hiddenLayers, deactivationFuncs)\n",
    "        for l in range(1,hiddenLayers+1):\n",
    "            Wl = cache['W'+str(l)]\n",
    "            gradients['dW'+str(l)] = gradients['dW'+str(l)] + (lamda/m) * Wl  # Regularization\n",
    "            Wl = Wl - alpha * gradients['dW'+str(l)]\n",
    "            cache['W'+str(l)] = Wl\n",
    "            bl = cache['b'+str(l)]\n",
    "            bl = bl - alpha * gradients['db'+str(l)]\n",
    "            cache['b'+str(l)] = bl\n",
    "        if(abs(old_cost - new_cost) < 0.00000000001):\n",
    "            print(\"breaking\" + str(old_cost) + str(new_cost))\n",
    "            break;\n",
    "        if(i%500 ==0):\n",
    "#             testcache = cache.copy()\n",
    "#             testcache['A0'] = Xtest\n",
    "#             ykey = 'A' + str(hiddenLayers+1)\n",
    "#             cache2 = forward_propagate(testcache, hiddenLayers, activationFuncs)\n",
    "#             ATestfinal = softmax(cache2['A'+str(hiddenLayers)])\n",
    "#             acc = get_accuracy(Ytest, ATestfinal)\n",
    "#             maxTestAcc = max(maxTestAcc, acc)\n",
    "            print (\"cost : \" + str(new_cost) + \" Iteration: \" + str(i))\n",
    "        cost_history.append(new_cost)\n",
    "    Afinal = softmax(Afinal) # Apply softmax to get the actual labels\n",
    "    cache['A'+str(hiddenLayers)] = Afinal\n",
    "    accuracy = get_accuracy(YOrg, Afinal)\n",
    "#     print(\"Maximum test accuracy is \" + str(maxTestAcc))\n",
    "    return cache, cost_history, new_cost, accuracy\n",
    "\n",
    "# Softmax activation function to get the probablity of the classes\n",
    "def softmax(z):\n",
    "    softMax = (np.exp(z) / np.sum(np.exp(z),axis=0))\n",
    "    softMax = np.matrix(np.argmax(softMax,axis=0)).T\n",
    "    return softMax\n",
    "\n",
    "def plotCostHistory(cost_history, alpha, i):\n",
    "     line = plt.plot(cost_history, label=alpha)\n",
    "     plt.ylabel('Cost');\n",
    "     plt.ylim( (0, 4) )\n",
    "     plt.xlabel('Iterations');\n",
    "     plt.title('Cost Progression with Iterations for different learning rates')\n",
    "     plt.legend()\n",
    "        \n",
    "def get_accuracy(Y, Ypred):\n",
    "    Y = np.matrix(Y)\n",
    "    numcorrect = 0\n",
    "    for (x,y) in zip(Ypred,Y):\n",
    "        if(x[0]==y[0]):\n",
    "            numcorrect+=1\n",
    "    accuracy=numcorrect*100.0/len(Y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the training data\n",
    "data_train = pd.read_csv('exam1_train.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "# Remove the un-necessary column\n",
    "df_train = data_train.drop('Unnamed: 0',axis=1)\n",
    "m = df_train.shape[0]\n",
    "\n",
    "y_train = pd.DataFrame(df_train['y'])\n",
    "X_train = df_train.drop(['y'], axis=1)\n",
    "\n",
    "X_train_mat = np.matrix(X_train).T\n",
    "y_train_mat = oneHotEncode(y_train).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y\n",
      "202  4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e2aa950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the test data\n",
    "data_test = pd.read_csv('exam1_test.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "# Remove the un-necessary column\n",
    "df_test = data_test.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "y_test = pd.DataFrame(df_test['y'])\n",
    "X_test = df_test.drop(['y'], axis=1)\n",
    "\n",
    "X_test_mat = np.matrix(X_test).T\n",
    "y_test_mat = oneHotEncode(y_test).T\n",
    "\n",
    "# Plot the selected pixel\n",
    "num = 202\n",
    "pixels = np.array(X_test[num:num+1], dtype='uint8')\n",
    "print(y_test[num:num+1])\n",
    "pixels = pixels.reshape((20, 20)).T\n",
    "plt.imshow(pixels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (10pts) Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize weights // TODO - change the initialization method\n",
    "\n",
    "np.random.seed(1) # Setting random seed to 1\n",
    "def initialize_parameters(hiddenLayers, layerSizes, activationFuncs):\n",
    "    cache = {}\n",
    "    for l in range(1,hiddenLayers+1):\n",
    "        Wl = 'W' + str(l)\n",
    "        bl = 'b' + str(l)\n",
    "        if(activationFuncs[l]=='sigmoid'):\n",
    "            print('activation is sigmoid for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * np.sqrt(2./layerSizes[l-1])\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        elif(activationFuncs[l]=='relu'):\n",
    "            print('activation is relu for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * (2./np.sqrt(layerSizes[l-1]))\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        else:\n",
    "            print('activation is random for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * 0.01 ## Random\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        l=l+1\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (30pts) Deep Neural Network model with more than 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of hidden layers including output layer are - 3\n"
     ]
    }
   ],
   "source": [
    "# Defining Hyperparameters\n",
    "hiddenLayers = 3\n",
    "layerSizes = [400, 100, 40, 10] # As specified in assignment requirements, can have as many hidden layers\n",
    "print(\" Number of hidden layers including output layer are - \" + str(hiddenLayers))\n",
    "activationFuncs = ['', 'relu', 'relu', 'sigmoid']\n",
    "deactivationFuncs = ['', 'relu_prime', 'relu_prime', 'sigmoid_prime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. (10pts) Predictions\n",
    "\n",
    "### 6. (20pts) Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimization function to check cost propagartion for different learning rates\n",
    "def optimize():\n",
    "    lamdas = [11.0]\n",
    "    alpha = [0.3]\n",
    "    i=0\n",
    "    scores = pd.DataFrame(columns=['alpha','cache','accuracy', 'lamda'])\n",
    "    print('*****************Training Data*********************')\n",
    "    for l in lamdas:\n",
    "        for a in alpha:\n",
    "            cache, cost_history, new_cost, acc = gradientDescent(X_train_mat, y_train_mat, y_train, a, 9601, \n",
    "                                                hiddenLayers, layerSizes, activationFuncs, deactivationFuncs, l, X_test_mat, y_test)\n",
    "            scores.loc[i] = pd.Series({'alpha':a, 'cache': cache, 'accuracy':acc, 'lamda':l})\n",
    "            print(\"Cost with \" + \"Alpha \" + str(a) + \" and lambda \" + str(l) + \" is \" + str(new_cost) + \" & \" + \"Accuracy is \" + str(acc) + \" %\")\n",
    "            plotCostHistory(cost_history, a, i)\n",
    "            i+=1\n",
    "        least_cost_comb = scores['accuracy'].idxmax()\n",
    "        alph = scores.iloc[[least_cost_comb]]['alpha'][least_cost_comb]\n",
    "        cache = scores.iloc[[least_cost_comb]]['cache'][least_cost_comb]\n",
    "        plt.show()\n",
    "        print('Best alpha is ' + str(alph))\n",
    "    return cache, alph,scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Training Data*********************\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 2\n",
      "activation is sigmoid for layer 3\n",
      "activation is sigmoid for layer 3\n",
      "cost : 8.31984513822 Iteration: 0\n",
      "cost : 0.277769161619 Iteration: 500\n",
      "cost : 0.21414548893 Iteration: 1000\n",
      "cost : 0.1996671085 Iteration: 1500\n",
      "cost : 0.193271140837 Iteration: 2000\n",
      "cost : 0.188039681037 Iteration: 2500\n",
      "cost : 0.186919637863 Iteration: 3000\n",
      "cost : 0.184982062439 Iteration: 3500\n",
      "cost : 0.180803423409 Iteration: 4000\n",
      "cost : 0.182460331389 Iteration: 4500\n",
      "cost : 0.179174902897 Iteration: 5000\n",
      "cost : 0.177107085101 Iteration: 5500\n",
      "cost : 0.176052602461 Iteration: 6000\n",
      "cost : 0.175093056676 Iteration: 6500\n",
      "cost : 0.174978477419 Iteration: 7000\n",
      "cost : 0.177876566497 Iteration: 7500\n",
      "cost : 0.173347404357 Iteration: 8000\n",
      "cost : 0.173257441114 Iteration: 8500\n",
      "cost : 0.172441773766 Iteration: 9000\n",
      "cost : 0.172469419788 Iteration: 9500\n",
      "Cost with Alpha 0.3 and lambda 11.0 is 0.173149927857 & Accuracy is 99.9428571429 %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEWCAYAAACe8xtsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbv8fs5+tRhUz1q8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112f5f250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha is 0.3\n"
     ]
    }
   ],
   "source": [
    "# Get the best combination of weights and alpha\n",
    "cache, alph,scores = optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Test Data************************\n",
      "Test Data Optimized accuracy is 94.8 for alpha 0.3 and lambda 11.0\n",
      "*********************************************\n",
      "Max alpha is 0.3\n",
      "Max lambda is 11.0\n"
     ]
    }
   ],
   "source": [
    "maxAcc = -sys.maxsize -1\n",
    "maxAlpha = None\n",
    "maxLambda = None\n",
    "for index, row in scores.iterrows():\n",
    "    a = row['alpha']\n",
    "    cache = row['cache']\n",
    "    lam = row['lamda']\n",
    "    cache['A0'] = X_test_mat\n",
    "    ykey = 'A' + str(hiddenLayers+1)\n",
    "    cache2 = forward_propagate(cache, hiddenLayers, activationFuncs)\n",
    "    Afinal = softmax(cache2['A'+str(hiddenLayers)])\n",
    "    acc = get_accuracy(y_test, Afinal)\n",
    "    if(acc > maxAcc):\n",
    "        maxAcc = acc\n",
    "        maxAlpha = a\n",
    "        maxLambda = lam\n",
    "    print('*****************Test Data************************')\n",
    "    print('Test Data Optimized accuracy is ' + str(acc) + ' for alpha ' + str(a) + ' and lambda ' + str(lam))\n",
    "print(\"*********************************************\")\n",
    "#print(\"Maximum accuracy so far is \" + str(maxAcc))\n",
    "print(\"Max alpha is \" + str(maxAlpha))\n",
    "print(\"Max lambda is \" + str(maxLambda))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
