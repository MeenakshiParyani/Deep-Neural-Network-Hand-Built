{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import sys\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (30pts) Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot Encoding the categorical output values to binary by adding 1's for that index and 0's otherwise\n",
    "def oneHotEncode(y):\n",
    "    enc = pd.get_dummies(y['y'])\n",
    "    return np.matrix(enc)\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while forward propagation\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do squishing on the linear function\n",
    "def sigmoid(z):\n",
    "    return 1./(1+np.exp(-z))\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while backward propagation to get gradients\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do undo the squishing on the linear function\n",
    "def sigmoid_prime(z):\n",
    "    inv = (np.exp(-z))/(np.power((1+np.exp(-z)),2))\n",
    "    return inv\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z,0)\n",
    "    \n",
    "def relu_prime(z):\n",
    "    return np.where(z < 0, 0.0, 1.0)\n",
    "\n",
    "# Get the loss of for the training example\n",
    "def get_cost(Y, Yhat, lamda, Wl):\n",
    "    m= Y.shape[1]\n",
    "    loss = np.multiply(np.log(Yhat),Y) + np.multiply((1.-Y), np.log(1. - Yhat))\n",
    "    loss = np.sum(loss)\n",
    "    cost = -1./m * np.sum(loss) + ((lamda/(2*m)) * np.sum(np.square(Wl)))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost\n",
    "\n",
    "# Forward propagation to calculate yHat by applying activation function twice\n",
    "def forward_propagate(cache, layerCount, activationFuncs):\n",
    "    for l in range(1, layerCount+1):\n",
    "        Zl =  'Z' + str(l)\n",
    "        Al =  'A' + str(l)\n",
    "        Wl =  'W' + str(l)\n",
    "        Al1 = 'A' + str(l-1)\n",
    "        bl =  'b' + str(l)\n",
    "        cache[Zl] = np.dot(cache[Wl], cache[Al1]) + cache[bl]\n",
    "        activationFunc = activationFuncs[l] + '(cache[Zl])'\n",
    "        cache[Al] = eval(activationFunc)\n",
    "    return cache\n",
    "\n",
    "# Backward Propagation function to calculate the gradients\n",
    "def back_propagate(cache, layerCount, deactivationFuncs):\n",
    "    gradients = {}\n",
    "    m = cache['A0'].shape[1]\n",
    "    for l in xrange(layerCount, 0, -1):\n",
    "        if(l==layerCount): # is last layer\n",
    "            dZl = cache['A' + str(l)] - cache['A' + str(l+1)]    \n",
    "        else:\n",
    "            Zl = 'Z'+ str(l)\n",
    "            dZl1 = 'dZ'+str(l+1)\n",
    "            deacFunc = deactivationFuncs[l] + '(cache[Zl])'\n",
    "            actv_prime = eval(deacFunc)\n",
    "            term1 = cache['W'+str(l+1)].T\n",
    "            term2 = gradients[dZl1]\n",
    "            product = np.dot( term1, term2)\n",
    "            dZl = np.multiply(product, actv_prime) \n",
    "        gradients['dZ' + str(l)] = dZl\n",
    "        Al1 = cache['A' + str(l-1)]\n",
    "#         print('--dZ'+str(l))\n",
    "#         print(dZl.shape)\n",
    "#         print('A'+str(l-1))\n",
    "#         print( Al1.shape)\n",
    "        dWl = (1./m) * np.dot(dZl, Al1.T)\n",
    "        dbl = (1./m) * np.sum(dZl, axis=1)\n",
    "#         print('dW' +str(l))\n",
    "#         print(dWl.shape)\n",
    "#         print('db' +str(l))\n",
    "#         print(dbl.shape)\n",
    "        gradients['dW' + str(l)] = dWl\n",
    "        gradients['db' + str(l)] = dbl\n",
    "    return gradients  \n",
    "\n",
    "\n",
    "def gradientDescent(X, Y, YOrg, alpha, iters, hiddenLayers, layerSizes, activationFuncs, deactivationFuncs, lamda, Xtest, Ytest):  \n",
    "    \n",
    "    cache = initialize_parameters(hiddenLayers, layerSizes,activationFuncs )\n",
    "    cache['A0'] = X\n",
    "    ykey = 'A' + str(hiddenLayers+1)\n",
    "    cache[ykey] = Y\n",
    "    old_cost = sys.maxsize\n",
    "    new_cost = sys.maxsize\n",
    "    cost_history = []\n",
    "    maxTestAcc = -sys.maxsize -1\n",
    "    for i in range(iters):\n",
    "        # Call Forward propagation to calculate yHat\n",
    "        cache = forward_propagate(cache, hiddenLayers, activationFuncs)\n",
    "        old_cost = new_cost\n",
    "        Afinal = cache['A'+str(hiddenLayers)]\n",
    "        new_cost = get_cost(Y, cache['A'+str(hiddenLayers)], lamda, cache['W'+str(hiddenLayers)]) # Regularization\n",
    "        gradients = back_propagate(cache, hiddenLayers, deactivationFuncs)\n",
    "        for l in range(1,hiddenLayers+1):\n",
    "            Wl = cache['W'+str(l)]\n",
    "            gradients['dW'+str(l)] = gradients['dW'+str(l)] + (lamda/m) * Wl  # Regularization\n",
    "            Wl = Wl - alpha * gradients['dW'+str(l)]\n",
    "            cache['W'+str(l)] = Wl\n",
    "            bl = cache['b'+str(l)]\n",
    "            bl = bl - alpha * gradients['db'+str(l)]\n",
    "            cache['b'+str(l)] = bl\n",
    "        if(abs(old_cost - new_cost) < 0.00000000001):\n",
    "            print(\"breaking\" + str(old_cost) + str(new_cost))\n",
    "            break;\n",
    "        if(i%10 ==0):\n",
    "            testcache = cache.copy()\n",
    "            testcache['A0'] = Xtest\n",
    "            ykey = 'A' + str(hiddenLayers+1)\n",
    "            cache2 = forward_propagate(testcache, hiddenLayers, activationFuncs)\n",
    "            ATestfinal = softmax(cache2['A'+str(hiddenLayers)])\n",
    "            acc = get_accuracy(Ytest, ATestfinal)\n",
    "            maxTestAcc = max(maxTestAcc, acc)\n",
    "            print (\"cost : \" + str(new_cost) + \" Test Accuracy : \" + str(acc) + \" Iteration: \" + str(i))\n",
    "        cost_history.append(new_cost)\n",
    "    Afinal = softmax(Afinal) # Apply softmax to get the actual labels\n",
    "    cache['A'+str(hiddenLayers)] = Afinal\n",
    "    accuracy = get_accuracy(YOrg, Afinal)\n",
    "    print(\"Maximum test accuracy is \" + str(maxTestAcc))\n",
    "    return cache, cost_history, new_cost, accuracy\n",
    "\n",
    "# Softmax activation function to get the probablity of the classes\n",
    "def softmax(z):\n",
    "    softMax = (np.exp(z) / np.sum(np.exp(z),axis=0))\n",
    "    softMax = np.matrix(np.argmax(softMax,axis=0)).T\n",
    "    return softMax\n",
    "\n",
    "def plotCostHistory(cost_history, alpha, i):\n",
    "     line = plt.plot(cost_history, label=alpha)\n",
    "     plt.ylabel('Cost');\n",
    "     plt.ylim( (0, 4) )\n",
    "     plt.xlabel('Iterations');\n",
    "     plt.title('Cost Progression with Iterations for different learning rates')\n",
    "     plt.legend()\n",
    "        \n",
    "def get_accuracy(Y, Ypred):\n",
    "    Y = np.matrix(Y)\n",
    "    numcorrect = 0\n",
    "    for (x,y) in zip(Ypred,Y):\n",
    "        if(x[0]==y[0]):\n",
    "            numcorrect+=1\n",
    "    accuracy=numcorrect*100.0/len(Y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the training data\n",
    "data_train = pd.read_csv('exam1_train.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "# Remove the un-necessary column\n",
    "df_train = data_train.drop('Unnamed: 0',axis=1)\n",
    "m = df_train.shape[0]\n",
    "\n",
    "y_train = pd.DataFrame(df_train['y'])\n",
    "X_train = df_train.drop(['y'], axis=1)\n",
    "\n",
    "X_train_mat = np.matrix(X_train).T\n",
    "y_train_mat = oneHotEncode(y_train).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y\n",
      "202  4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f40d950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the test data\n",
    "data_test = pd.read_csv('exam1_test.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "# Remove the un-necessary column\n",
    "df_test = data_test.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "y_test = pd.DataFrame(df_test['y'])\n",
    "X_test = df_test.drop(['y'], axis=1)\n",
    "\n",
    "X_test_mat = np.matrix(X_test).T\n",
    "y_test_mat = oneHotEncode(y_test).T\n",
    "\n",
    "# Plot the selected pixel\n",
    "num = 202\n",
    "pixels = np.array(X_test[num:num+1], dtype='uint8')\n",
    "print(y_test[num:num+1])\n",
    "pixels = pixels.reshape((20, 20)).T\n",
    "plt.imshow(pixels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (10pts) Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize weights // TODO - change the initialization method\n",
    "\n",
    "np.random.seed(1) # Setting random seed to 1\n",
    "def initialize_parameters(hiddenLayers, layerSizes, activationFuncs):\n",
    "    cache = {}\n",
    "    for l in range(1,hiddenLayers+1):\n",
    "        #print(l)\n",
    "        Wl = 'W' + str(l)\n",
    "        bl = 'b' + str(l)\n",
    "        #print(layerSizes[l])\n",
    "        #print(layerSizes[l-1])\n",
    "        print('activation is ' + str(activationFuncs[l]) + ' for layer ' + str(l))\n",
    "        if(activationFuncs[l]=='sigmoid'):\n",
    "            print('activation is sigmoid for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * np.sqrt(2./layerSizes[l-1])\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        elif(activationFuncs[l]=='relu'):\n",
    "            print('activation is relu for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * (2./np.sqrt(layerSizes[l-1]))\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        else:\n",
    "            print('activation is random for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * 0.01 ## Random\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        l=l+1\n",
    "    #print(cache)\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (30pts) Deep Neural Network model with more than 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of hidden layers including output layer are - 3\n"
     ]
    }
   ],
   "source": [
    "# Defining Hyperparameters\n",
    "hiddenLayers = 3\n",
    "layerSizes = [400, 100, 40, 10] # As specified in assignment requirements, can have as many hidden layers\n",
    "print(\" Number of hidden layers including output layer are - \" + str(hiddenLayers))\n",
    "activationFuncs = ['', 'relu', 'relu', 'sigmoid']\n",
    "deactivationFuncs = ['', 'relu_prime', 'relu_prime', 'sigmoid_prime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cache = initialize_parameters(hiddenLayers, layerSizes, activationFuncs)\n",
    "# print(cache['W1'].shape)\n",
    "# # print(cache['W2'].shape)\n",
    "# # print(cache['W3'].shape)\n",
    "# # print(cache['b1'].shape)\n",
    "# # print(cache['b2'].shape)\n",
    "# # print(cache['b3'].shape)\n",
    "# cache['A0'] = X_train_mat\n",
    "# ykey = 'A' + str(len(layerSizes))\n",
    "# cache[ykey] = y_train_mat\n",
    "# cache = forward_propagate(cache, hiddenLayers, ['', 'apply_sigmoid', 'apply_sigmoid','apply_sigmoid'])\n",
    "# # print(cache['A1'])\n",
    "# # print(cache['A2'])\n",
    "# # print(cache['A3'])\n",
    "# # print(cache['Z2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# gradients = back_propagate(cache, hiddenLayers, deactivationFuncs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. (10pts) Predictions\n",
    "\n",
    "### 6. (20pts) Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimization function to check cost propagartion for different learning rates\n",
    "def optimize():\n",
    "    lamdas = [14.0]\n",
    "    alpha = [0.3,0.4,0.5,0.6]\n",
    "    i=0\n",
    "    scores = pd.DataFrame(columns=['alpha','cache','accuracy', 'lamda'])\n",
    "    print('*****************Training Data*********************')\n",
    "    for l in lamdas:\n",
    "        for a in alpha:\n",
    "            cache, cost_history, new_cost, acc = gradientDescent(X_train_mat, y_train_mat, y_train, a, 15000, \n",
    "                                                hiddenLayers, layerSizes, activationFuncs, deactivationFuncs, l, X_test_mat, y_test)\n",
    "            scores.loc[i] = pd.Series({'alpha':a, 'cache': cache, 'accuracy':acc, 'lamda':l})\n",
    "            print(\"Cost with \" + \"Alpha \" + str(a) + \" and lambda \" + str(l) + \" is \" + str(new_cost) + \" & \" + \"Accuracy is \" + str(acc) + \" %\")\n",
    "            plotCostHistory(cost_history, a, i)\n",
    "            i+=1\n",
    "        least_cost_comb = scores['accuracy'].idxmax()\n",
    "        alph = scores.iloc[[least_cost_comb]]['alpha'][least_cost_comb]\n",
    "        cache = scores.iloc[[least_cost_comb]]['cache'][least_cost_comb]\n",
    "        plt.show()\n",
    "        print('Best alpha is ' + str(alph))\n",
    "    return cache, alph,scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Training Data*********************\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 2\n",
      "activation is sigmoid for layer 3\n",
      "activation is sigmoid for layer 3\n",
      "cost : 8.32843109401 Test Accuracy : 10.7333333333 Iteration: 0\n",
      "cost : 2.98364050691 Test Accuracy : 51.5333333333 Iteration: 10\n",
      "cost : 1.88824945209 Test Accuracy : 72.2 Iteration: 20\n",
      "cost : 1.95258757419 Test Accuracy : 71.1333333333 Iteration: 30\n",
      "cost : 1.20393569582 Test Accuracy : 80.6666666667 Iteration: 40\n",
      "cost : 0.995012783609 Test Accuracy : 84.0666666667 Iteration: 50\n",
      "cost : 0.935227227517 Test Accuracy : 84.0 Iteration: 60\n",
      "cost : 0.835637293229 Test Accuracy : 84.5333333333 Iteration: 70\n",
      "cost : 0.722066096967 Test Accuracy : 87.6666666667 Iteration: 80\n",
      "cost : 0.690261785726 Test Accuracy : 87.7333333333 Iteration: 90\n",
      "cost : 0.664105014915 Test Accuracy : 88.9333333333 Iteration: 100\n",
      "cost : 0.605895544986 Test Accuracy : 90.0666666667 Iteration: 110\n",
      "cost : 0.632662474412 Test Accuracy : 88.8 Iteration: 120\n",
      "cost : 0.561482173734 Test Accuracy : 90.2666666667 Iteration: 130\n",
      "cost : 0.53387031082 Test Accuracy : 90.4 Iteration: 140\n",
      "cost : 0.516066577009 Test Accuracy : 90.8666666667 Iteration: 150\n",
      "cost : 0.502648343046 Test Accuracy : 91.2 Iteration: 160\n",
      "cost : 0.49575712659 Test Accuracy : 90.9333333333 Iteration: 170\n",
      "cost : 0.490162735124 Test Accuracy : 91.0 Iteration: 180\n",
      "cost : 0.469788588163 Test Accuracy : 91.3333333333 Iteration: 190\n",
      "cost : 0.453818117221 Test Accuracy : 91.2 Iteration: 200\n",
      "cost : 0.443102527882 Test Accuracy : 91.1333333333 Iteration: 210\n",
      "cost : 0.434493682232 Test Accuracy : 91.1333333333 Iteration: 220\n",
      "cost : 0.428021477635 Test Accuracy : 91.2 Iteration: 230\n",
      "cost : 0.426959023729 Test Accuracy : 91.5333333333 Iteration: 240\n",
      "cost : 0.420579217099 Test Accuracy : 91.7333333333 Iteration: 250\n",
      "cost : 0.40711985747 Test Accuracy : 91.4666666667 Iteration: 260\n",
      "cost : 0.398106546302 Test Accuracy : 91.6666666667 Iteration: 270\n",
      "cost : 0.393017904407 Test Accuracy : 91.8 Iteration: 280\n",
      "cost : 0.395388118814 Test Accuracy : 91.8 Iteration: 290\n",
      "cost : 0.406256183298 Test Accuracy : 91.6 Iteration: 300\n",
      "cost : 0.386511941323 Test Accuracy : 91.6 Iteration: 310\n",
      "cost : 0.370428378523 Test Accuracy : 91.7333333333 Iteration: 320\n",
      "cost : 0.364947759151 Test Accuracy : 91.8666666667 Iteration: 330\n",
      "cost : 0.363591602759 Test Accuracy : 91.9333333333 Iteration: 340\n",
      "cost : 0.364988915023 Test Accuracy : 91.9333333333 Iteration: 350\n",
      "cost : 0.359900026972 Test Accuracy : 92.0 Iteration: 360\n",
      "cost : 0.351584206744 Test Accuracy : 91.8666666667 Iteration: 370\n",
      "cost : 0.346751836041 Test Accuracy : 92.1333333333 Iteration: 380\n",
      "cost : 0.344658685101 Test Accuracy : 92.0 Iteration: 390\n",
      "cost : 0.343359256877 Test Accuracy : 92.0 Iteration: 400\n",
      "cost : 0.340356562906 Test Accuracy : 92.0 Iteration: 410\n",
      "cost : 0.335428867368 Test Accuracy : 92.3333333333 Iteration: 420\n",
      "cost : 0.332239330544 Test Accuracy : 92.4666666667 Iteration: 430\n",
      "cost : 0.331808852338 Test Accuracy : 92.6 Iteration: 440\n",
      "cost : 0.330799526638 Test Accuracy : 92.7333333333 Iteration: 450\n",
      "cost : 0.326687575991 Test Accuracy : 92.8 Iteration: 460\n",
      "cost : 0.323915535954 Test Accuracy : 92.9333333333 Iteration: 470\n",
      "cost : 0.321708331952 Test Accuracy : 93.0666666667 Iteration: 480\n",
      "cost : 0.317748766737 Test Accuracy : 93.0 Iteration: 490\n",
      "cost : 0.314847536343 Test Accuracy : 92.8666666667 Iteration: 500\n",
      "cost : 0.315711261651 Test Accuracy : 92.6 Iteration: 510\n",
      "cost : 0.319932900254 Test Accuracy : 92.8 Iteration: 520\n",
      "cost : 0.313533206525 Test Accuracy : 92.6 Iteration: 530\n",
      "cost : 0.303899829821 Test Accuracy : 92.7333333333 Iteration: 540\n",
      "cost : 0.301016297427 Test Accuracy : 92.6 Iteration: 550\n",
      "cost : 0.301038831882 Test Accuracy : 92.4666666667 Iteration: 560\n",
      "cost : 0.3040118915 Test Accuracy : 92.5333333333 Iteration: 570\n",
      "cost : 0.304249709853 Test Accuracy : 92.4666666667 Iteration: 580\n",
      "cost : 0.297778175757 Test Accuracy : 92.5333333333 Iteration: 590\n",
      "cost : 0.294104079809 Test Accuracy : 92.6666666667 Iteration: 600\n",
      "cost : 0.294613869391 Test Accuracy : 93.0 Iteration: 610\n",
      "cost : 0.298018333944 Test Accuracy : 92.9333333333 Iteration: 620\n",
      "cost : 0.297063495863 Test Accuracy : 92.9333333333 Iteration: 630\n",
      "cost : 0.28900251565 Test Accuracy : 93.0 Iteration: 640\n",
      "cost : 0.285551929693 Test Accuracy : 92.8666666667 Iteration: 650\n",
      "cost : 0.284818383074 Test Accuracy : 93.0666666667 Iteration: 660\n",
      "cost : 0.285966910155 Test Accuracy : 92.9333333333 Iteration: 670\n",
      "cost : 0.287084059038 Test Accuracy : 93.3333333333 Iteration: 680\n",
      "cost : 0.285108691039 Test Accuracy : 93.4666666667 Iteration: 690\n",
      "cost : 0.281322055939 Test Accuracy : 93.2 Iteration: 700\n",
      "cost : 0.280155119349 Test Accuracy : 93.0666666667 Iteration: 710\n",
      "cost : 0.281779743028 Test Accuracy : 92.9333333333 Iteration: 720\n",
      "cost : 0.284185172626 Test Accuracy : 92.8666666667 Iteration: 730\n",
      "cost : 0.280625769465 Test Accuracy : 92.9333333333 Iteration: 740\n",
      "cost : 0.274591644921 Test Accuracy : 93.2666666667 Iteration: 750\n",
      "cost : 0.273096285765 Test Accuracy : 93.0 Iteration: 760\n",
      "cost : 0.274216281144 Test Accuracy : 92.7333333333 Iteration: 770\n",
      "cost : 0.27714288944 Test Accuracy : 92.6 Iteration: 780\n",
      "cost : 0.276172441512 Test Accuracy : 92.6 Iteration: 790\n",
      "cost : 0.272034211851 Test Accuracy : 92.8 Iteration: 800\n",
      "cost : 0.270182555582 Test Accuracy : 93.1333333333 Iteration: 810\n",
      "cost : 0.272465932551 Test Accuracy : 93.0666666667 Iteration: 820\n",
      "cost : 0.277176316093 Test Accuracy : 93.0666666667 Iteration: 830\n",
      "cost : 0.275739156304 Test Accuracy : 93.0666666667 Iteration: 840\n",
      "cost : 0.266251328933 Test Accuracy : 93.2 Iteration: 850\n",
      "cost : 0.264677426897 Test Accuracy : 93.0666666667 Iteration: 860\n",
      "cost : 0.265009854516 Test Accuracy : 92.9333333333 Iteration: 870\n",
      "cost : 0.266724197424 Test Accuracy : 92.8 Iteration: 880\n",
      "cost : 0.26740388207 Test Accuracy : 92.8 Iteration: 890\n",
      "cost : 0.264808876131 Test Accuracy : 92.8666666667 Iteration: 900\n",
      "cost : 0.26229020815 Test Accuracy : 93.0 Iteration: 910\n",
      "cost : 0.262829299298 Test Accuracy : 93.2666666667 Iteration: 920\n",
      "cost : 0.265147628033 Test Accuracy : 93.2666666667 Iteration: 930\n",
      "cost : 0.266431868076 Test Accuracy : 93.2 Iteration: 940\n",
      "cost : 0.263996400793 Test Accuracy : 93.4666666667 Iteration: 950\n",
      "cost : 0.260843544056 Test Accuracy : 93.5333333333 Iteration: 960\n",
      "cost : 0.259472486689 Test Accuracy : 93.6 Iteration: 970\n",
      "cost : 0.25948097299 Test Accuracy : 93.5333333333 Iteration: 980\n",
      "cost : 0.260429965476 Test Accuracy : 93.6 Iteration: 990\n",
      "cost : 0.260281944389 Test Accuracy : 93.6 Iteration: 1000\n",
      "cost : 0.258629122869 Test Accuracy : 93.4666666667 Iteration: 1010\n",
      "cost : 0.258167822186 Test Accuracy : 93.2666666667 Iteration: 1020\n",
      "cost : 0.259728626379 Test Accuracy : 93.4 Iteration: 1030\n",
      "cost : 0.261069764748 Test Accuracy : 93.4 Iteration: 1040\n",
      "cost : 0.259715316624 Test Accuracy : 93.3333333333 Iteration: 1050\n",
      "cost : 0.256357870901 Test Accuracy : 93.3333333333 Iteration: 1060\n",
      "cost : 0.254718839147 Test Accuracy : 93.2666666667 Iteration: 1070\n",
      "cost : 0.254077799667 Test Accuracy : 93.1333333333 Iteration: 1080\n",
      "cost : 0.254491207839 Test Accuracy : 93.0666666667 Iteration: 1090\n",
      "cost : 0.255644736802 Test Accuracy : 93.0666666667 Iteration: 1100\n",
      "cost : 0.255250541528 Test Accuracy : 93.0 Iteration: 1110\n",
      "cost : 0.253664347444 Test Accuracy : 93.3333333333 Iteration: 1120\n",
      "cost : 0.253959332317 Test Accuracy : 93.5333333333 Iteration: 1130\n",
      "cost : 0.257317861307 Test Accuracy : 93.5333333333 Iteration: 1140\n",
      "cost : 0.259861819897 Test Accuracy : 93.5333333333 Iteration: 1150\n",
      "cost : 0.254699102809 Test Accuracy : 93.8666666667 Iteration: 1160\n",
      "cost : 0.251144055762 Test Accuracy : 93.8 Iteration: 1170\n",
      "cost : 0.25007014234 Test Accuracy : 93.6666666667 Iteration: 1180\n",
      "cost : 0.24986063547 Test Accuracy : 93.7333333333 Iteration: 1190\n",
      "cost : 0.250802196656 Test Accuracy : 93.8 Iteration: 1200\n",
      "cost : 0.252347095252 Test Accuracy : 93.7333333333 Iteration: 1210\n",
      "cost : 0.253028711399 Test Accuracy : 93.7333333333 Iteration: 1220\n",
      "cost : 0.250602686997 Test Accuracy : 93.8666666667 Iteration: 1230\n",
      "cost : 0.248357356535 Test Accuracy : 93.7333333333 Iteration: 1240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 0.247723050792 Test Accuracy : 93.6666666667 Iteration: 1250\n",
      "cost : 0.249470469471 Test Accuracy : 93.7333333333 Iteration: 1260\n",
      "cost : 0.254035887123 Test Accuracy : 93.8 Iteration: 1270\n",
      "cost : 0.254802343815 Test Accuracy : 93.8 Iteration: 1280\n",
      "cost : 0.247211980123 Test Accuracy : 93.6666666667 Iteration: 1290\n",
      "cost : 0.245957814459 Test Accuracy : 93.5333333333 Iteration: 1300\n",
      "cost : 0.246791474364 Test Accuracy : 93.3333333333 Iteration: 1310\n",
      "cost : 0.24941037591 Test Accuracy : 93.1333333333 Iteration: 1320\n",
      "cost : 0.253630113167 Test Accuracy : 93.2 Iteration: 1330\n",
      "cost : 0.25814712784 Test Accuracy : 93.1333333333 Iteration: 1340\n",
      "cost : 0.249055113718 Test Accuracy : 93.4666666667 Iteration: 1350\n",
      "cost : 0.245758776281 Test Accuracy : 93.5333333333 Iteration: 1360\n",
      "cost : 0.246885951787 Test Accuracy : 93.6 Iteration: 1370\n",
      "cost : 0.248302741592 Test Accuracy : 93.6 Iteration: 1380\n",
      "cost : 0.246158109982 Test Accuracy : 93.6 Iteration: 1390\n",
      "cost : 0.243635713104 Test Accuracy : 93.4 Iteration: 1400\n",
      "cost : 0.243075584984 Test Accuracy : 93.4 Iteration: 1410\n",
      "cost : 0.243538759151 Test Accuracy : 93.4 Iteration: 1420\n",
      "cost : 0.244172226088 Test Accuracy : 93.5333333333 Iteration: 1430\n",
      "cost : 0.244366222301 Test Accuracy : 93.6 Iteration: 1440\n",
      "cost : 0.243629809064 Test Accuracy : 93.5333333333 Iteration: 1450\n",
      "cost : 0.242973773732 Test Accuracy : 93.4666666667 Iteration: 1460\n",
      "cost : 0.24416716502 Test Accuracy : 93.5333333333 Iteration: 1470\n",
      "cost : 0.249076802411 Test Accuracy : 93.2666666667 Iteration: 1480\n",
      "cost : 0.253979989741 Test Accuracy : 93.3333333333 Iteration: 1490\n",
      "cost : 0.245588192815 Test Accuracy : 93.4 Iteration: 1500\n",
      "cost : 0.241319980421 Test Accuracy : 93.4666666667 Iteration: 1510\n",
      "cost : 0.240898403282 Test Accuracy : 93.4 Iteration: 1520\n",
      "cost : 0.241386042312 Test Accuracy : 93.6 Iteration: 1530\n",
      "cost : 0.241903251934 Test Accuracy : 93.3333333333 Iteration: 1540\n",
      "cost : 0.241491732708 Test Accuracy : 93.6 Iteration: 1550\n",
      "cost : 0.241742892499 Test Accuracy : 93.6666666667 Iteration: 1560\n",
      "cost : 0.24422274293 Test Accuracy : 94.0 Iteration: 1570\n",
      "cost : 0.246923113681 Test Accuracy : 93.8666666667 Iteration: 1580\n",
      "cost : 0.243973715873 Test Accuracy : 94.0 Iteration: 1590\n",
      "cost : 0.241821044111 Test Accuracy : 93.9333333333 Iteration: 1600\n",
      "cost : 0.241281779578 Test Accuracy : 93.8666666667 Iteration: 1610\n",
      "cost : 0.240839438502 Test Accuracy : 93.9333333333 Iteration: 1620\n"
     ]
    }
   ],
   "source": [
    "# Get the best combination of weights and alpha\n",
    "cache, alph,scores = optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maxAcc = -sys.maxsize -1\n",
    "maxAlpha = None\n",
    "maxLambda = None\n",
    "for index, row in scores.iterrows():\n",
    "    a = row['alpha']\n",
    "    cache = row['cache']\n",
    "    lam = row['lamda']\n",
    "    cache['A0'] = X_test_mat\n",
    "    ykey = 'A' + str(hiddenLayers+1)\n",
    "    cache2 = forward_propagate(cache, hiddenLayers, activationFuncs)\n",
    "    Afinal = softmax(cache2['A'+str(hiddenLayers)])\n",
    "    acc = get_accuracy(y_test, Afinal)\n",
    "    if(acc > maxAcc):\n",
    "        maxAcc = acc\n",
    "        maxAlpha = a\n",
    "        maxLambda = lam\n",
    "    print('*****************Test Data************************')\n",
    "    print('Test Data Optimized accuracy is ' + str(acc) + ' for alpha ' + str(a) + ' and lambda ' + str(lam))\n",
    "print(\"*********************************************\")\n",
    "print(\"Maximum accuracy so far is \" + str(maxAcc))\n",
    "print(\"Max alpha is \" + str(maxAlpha))\n",
    "print(\"Max lambda is \" + str(maxLambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
