{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import sys\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (30pts) Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot Encoding the categorical output values to binary by adding 1's for that index and 0's otherwise\n",
    "def oneHotEncode(y):\n",
    "    enc = pd.get_dummies(y['y'])\n",
    "    return np.matrix(enc)\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while forward propagation\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do squishing on the linear function\n",
    "def sigmoid(z):\n",
    "    return 1./(1+np.exp(-z))\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while backward propagation to get gradients\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do undo the squishing on the linear function\n",
    "def sigmoid_prime(z):\n",
    "    inv = (np.exp(-z))/(np.power((1+np.exp(-z)),2))\n",
    "    return inv\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z,0)\n",
    "    \n",
    "def relu_prime(z):\n",
    "    return np.where(z < 0, 0.0, 1.0)\n",
    "\n",
    "# Get the loss of for the training example\n",
    "def get_cost(Y, Yhat, lamda, Wl):\n",
    "    m= Y.shape[1]\n",
    "    loss = np.multiply(np.log(Yhat),Y) + np.multiply((1.-Y), np.log(1. - Yhat))\n",
    "    loss = np.sum(loss)\n",
    "    cost = -1./m * np.sum(loss) + ((lamda/(2*m)) * np.sum(np.square(Wl)))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost\n",
    "\n",
    "# Forward propagation to calculate yHat by applying activation function twice\n",
    "def forward_propagate(cache, layerCount, activationFuncs):\n",
    "    for l in range(1, layerCount+1):\n",
    "        Zl =  'Z' + str(l)\n",
    "        Al =  'A' + str(l)\n",
    "        Wl =  'W' + str(l)\n",
    "        Al1 = 'A' + str(l-1)\n",
    "        bl =  'b' + str(l)\n",
    "        cache[Zl] = np.dot(cache[Wl], cache[Al1]) + cache[bl]\n",
    "        activationFunc = activationFuncs[l] + '(cache[Zl])'\n",
    "        cache[Al] = eval(activationFunc)\n",
    "    return cache\n",
    "\n",
    "# Backward Propagation function to calculate the gradients\n",
    "def back_propagate(cache, layerCount, deactivationFuncs):\n",
    "    gradients = {}\n",
    "    m = cache['A0'].shape[1]\n",
    "    for l in xrange(layerCount, 0, -1):\n",
    "        if(l==layerCount): # is last layer\n",
    "            dZl = cache['A' + str(l)] - cache['A' + str(l+1)]    \n",
    "        else:\n",
    "            Zl = 'Z'+ str(l)\n",
    "            dZl1 = 'dZ'+str(l+1)\n",
    "            deacFunc = deactivationFuncs[l] + '(cache[Zl])'\n",
    "            actv_prime = eval(deacFunc)\n",
    "            term1 = cache['W'+str(l+1)].T\n",
    "            term2 = gradients[dZl1]\n",
    "            product = np.dot( term1, term2)\n",
    "            dZl = np.multiply(product, actv_prime) \n",
    "        gradients['dZ' + str(l)] = dZl\n",
    "        Al1 = cache['A' + str(l-1)]\n",
    "#         print('--dZ'+str(l))\n",
    "#         print(dZl.shape)\n",
    "#         print('A'+str(l-1))\n",
    "#         print( Al1.shape)\n",
    "        dWl = (1./m) * np.dot(dZl, Al1.T)\n",
    "        dbl = (1./m) * np.sum(dZl, axis=1)\n",
    "#         print('dW' +str(l))\n",
    "#         print(dWl.shape)\n",
    "#         print('db' +str(l))\n",
    "#         print(dbl.shape)\n",
    "        gradients['dW' + str(l)] = dWl\n",
    "        gradients['db' + str(l)] = dbl\n",
    "    return gradients  \n",
    "\n",
    "\n",
    "def gradientDescent(X, Y, YOrg, alpha, iters, hiddenLayers, layerSizes, activationFuncs, deactivationFuncs, lamda, Xtest, Ytest):  \n",
    "    \n",
    "    cache = initialize_parameters(hiddenLayers, layerSizes,activationFuncs )\n",
    "    cache['A0'] = X\n",
    "    ykey = 'A' + str(hiddenLayers+1)\n",
    "    cache[ykey] = Y\n",
    "    old_cost = sys.maxsize\n",
    "    new_cost = sys.maxsize\n",
    "    cost_history = []\n",
    "    for i in range(iters):\n",
    "        # Call Forward propagation to calculate yHat\n",
    "        cache = forward_propagate(cache, hiddenLayers, activationFuncs)\n",
    "        old_cost = new_cost\n",
    "        Afinal = cache['A'+str(hiddenLayers)]\n",
    "        new_cost = get_cost(Y, cache['A'+str(hiddenLayers)], lamda, cache['W'+str(hiddenLayers)]) # Regularization\n",
    "        gradients = back_propagate(cache, hiddenLayers, deactivationFuncs)\n",
    "        for l in range(1,hiddenLayers+1):\n",
    "            Wl = cache['W'+str(l)]\n",
    "            gradients['dW'+str(l)] = gradients['dW'+str(l)] + (lamda/m) * Wl  # Regularization\n",
    "            Wl = Wl - alpha * gradients['dW'+str(l)]\n",
    "            cache['W'+str(l)] = Wl\n",
    "            bl = cache['b'+str(l)]\n",
    "            bl = bl - alpha * gradients['db'+str(l)]\n",
    "            cache['b'+str(l)] = bl\n",
    "        if(abs(old_cost - new_cost) < 0.00000001):\n",
    "            print(\"breaking\" + str(old_cost) + str(new_cost))\n",
    "            break;\n",
    "        if(i%100 ==0):\n",
    "            testcache = cache.copy()\n",
    "            testcache['A0'] = Xtest\n",
    "            ykey = 'A' + str(hiddenLayers+1)\n",
    "            cache2 = forward_propagate(testcache, hiddenLayers, activationFuncs)\n",
    "            ATestfinal = softmax(cache2['A'+str(hiddenLayers)])\n",
    "            acc = get_accuracy(Ytest, ATestfinal)\n",
    "            print (\"cost : \" + str(new_cost) + \" Test Accuracy : \" + str(acc) + \" Iteration: \" + str(i))\n",
    "        cost_history.append(new_cost)\n",
    "    Afinal = softmax(Afinal) # Apply softmax to get the actual labels\n",
    "    cache['A'+str(hiddenLayers)] = Afinal\n",
    "    accuracy = get_accuracy(YOrg, Afinal)\n",
    "    return cache, cost_history, new_cost, accuracy\n",
    "\n",
    "# Softmax activation function to get the probablity of the classes\n",
    "def softmax(z):\n",
    "    softMax = (np.exp(z) / np.sum(np.exp(z),axis=0))\n",
    "    softMax = np.matrix(np.argmax(softMax,axis=0)).T\n",
    "    return softMax\n",
    "\n",
    "def plotCostHistory(cost_history, alpha, i):\n",
    "     line = plt.plot(cost_history, label=alpha)\n",
    "     plt.ylabel('Cost');\n",
    "     plt.ylim( (0, 4) )\n",
    "     plt.xlabel('Iterations');\n",
    "     plt.title('Cost Progression with Iterations for different learning rates')\n",
    "     plt.legend()\n",
    "        \n",
    "def get_accuracy(Y, Ypred):\n",
    "    Y = np.matrix(Y)\n",
    "    numcorrect = 0\n",
    "    for (x,y) in zip(Ypred,Y):\n",
    "        if(x[0]==y[0]):\n",
    "            numcorrect+=1\n",
    "    accuracy=numcorrect*100.0/len(Y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the training data\n",
    "data_train = pd.read_csv('exam1_train.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "# Remove the un-necessary column\n",
    "df_train = data_train.drop('Unnamed: 0',axis=1)\n",
    "m = df_train.shape[0]\n",
    "\n",
    "y_train = pd.DataFrame(df_train['y'])\n",
    "X_train = df_train.drop(['y'], axis=1)\n",
    "\n",
    "X_train_mat = np.matrix(X_train).T\n",
    "y_train_mat = oneHotEncode(y_train).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y\n",
      "202  4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a64b950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the test data\n",
    "data_test = pd.read_csv('exam1_test.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "# Remove the un-necessary column\n",
    "df_test = data_test.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "y_test = pd.DataFrame(df_test['y'])\n",
    "X_test = df_test.drop(['y'], axis=1)\n",
    "\n",
    "X_test_mat = np.matrix(X_test).T\n",
    "y_test_mat = oneHotEncode(y_test).T\n",
    "\n",
    "# Plot the selected pixel\n",
    "num = 202\n",
    "pixels = np.array(X_test[num:num+1], dtype='uint8')\n",
    "print(y_test[num:num+1])\n",
    "pixels = pixels.reshape((20, 20)).T\n",
    "plt.imshow(pixels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (10pts) Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize weights // TODO - change the initialization method\n",
    "\n",
    "np.random.seed(1) # Setting random seed to 1\n",
    "def initialize_parameters(hiddenLayers, layerSizes, activationFuncs):\n",
    "    cache = {}\n",
    "    for l in range(1,hiddenLayers+1):\n",
    "        #print(l)\n",
    "        Wl = 'W' + str(l)\n",
    "        bl = 'b' + str(l)\n",
    "        #print(layerSizes[l])\n",
    "        #print(layerSizes[l-1])\n",
    "        print('activation is ' + str(activationFuncs[l]) + ' for layer ' + str(l))\n",
    "        if(activationFuncs[l]=='sigmoid'):\n",
    "            print('activation is sigmoid for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * np.sqrt(2./layerSizes[l-1])\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        elif(activationFuncs[l]=='relu'):\n",
    "            print('activation is relu for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * (2./np.sqrt(layerSizes[l-1]))\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        else:\n",
    "            print('activation is random for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * 0.01 ## Random\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        l=l+1\n",
    "    #print(cache)\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (30pts) Deep Neural Network model with more than 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of hidden layers including output layer are - 3\n"
     ]
    }
   ],
   "source": [
    "# Defining Hyperparameters\n",
    "hiddenLayers = 3\n",
    "layerSizes = [400, 100, 40, 10] # As specified in assignment requirements, can have as many hidden layers\n",
    "print(\" Number of hidden layers including output layer are - \" + str(hiddenLayers))\n",
    "activationFuncs = ['', 'relu', 'relu', 'sigmoid']\n",
    "deactivationFuncs = ['', 'relu_prime', 'relu_prime', 'sigmoid_prime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cache = initialize_parameters(hiddenLayers, layerSizes, activationFuncs)\n",
    "# print(cache['W1'].shape)\n",
    "# # print(cache['W2'].shape)\n",
    "# # print(cache['W3'].shape)\n",
    "# # print(cache['b1'].shape)\n",
    "# # print(cache['b2'].shape)\n",
    "# # print(cache['b3'].shape)\n",
    "# cache['A0'] = X_train_mat\n",
    "# ykey = 'A' + str(len(layerSizes))\n",
    "# cache[ykey] = y_train_mat\n",
    "# cache = forward_propagate(cache, hiddenLayers, ['', 'apply_sigmoid', 'apply_sigmoid','apply_sigmoid'])\n",
    "# # print(cache['A1'])\n",
    "# # print(cache['A2'])\n",
    "# # print(cache['A3'])\n",
    "# # print(cache['Z2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# gradients = back_propagate(cache, hiddenLayers, deactivationFuncs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. (10pts) Predictions\n",
    "\n",
    "### 6. (20pts) Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimization function to check cost propagartion for different learning rates\n",
    "def optimize():\n",
    "    lamdas = [10.0]\n",
    "    alpha = [0.2]\n",
    "    i=0\n",
    "    scores = pd.DataFrame(columns=['alpha','cache','accuracy', 'lamda'])\n",
    "    print('*****************Training Data*********************')\n",
    "    for l in lamdas:\n",
    "        for a in alpha:\n",
    "            cache, cost_history, new_cost, acc = gradientDescent(X_train_mat, y_train_mat, y_train, a, 8000, \n",
    "                                                hiddenLayers, layerSizes, activationFuncs, deactivationFuncs, l, X_test_mat, y_test)\n",
    "            scores.loc[i] = pd.Series({'alpha':a, 'cache': cache, 'accuracy':acc, 'lamda':l})\n",
    "            print(\"Cost with \" + \"Alpha \" + str(a) + \" and lambda \" + str(l) + \" is \" + str(new_cost) + \" & \" + \"Accuracy is \" + str(acc) + \" %\")\n",
    "            plotCostHistory(cost_history, a, i)\n",
    "            i+=1\n",
    "        least_cost_comb = scores['accuracy'].idxmax()\n",
    "        alph = scores.iloc[[least_cost_comb]]['alpha'][least_cost_comb]\n",
    "        cache = scores.iloc[[least_cost_comb]]['cache'][least_cost_comb]\n",
    "        plt.show()\n",
    "        print('Best alpha is ' + str(alph))\n",
    "    return cache, alph,scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Training Data*********************\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 2\n",
      "activation is sigmoid for layer 3\n",
      "activation is sigmoid for layer 3\n",
      "cost : 8.31698315296 Test Accuracy : 11.0666666667 Iteration: 0\n",
      "cost : 0.674380883014 Test Accuracy : 87.3333333333 Iteration: 100\n",
      "cost : 0.479245154929 Test Accuracy : 90.4 Iteration: 200\n",
      "cost : 0.394215509003 Test Accuracy : 90.7333333333 Iteration: 300\n",
      "cost : 0.341360980847 Test Accuracy : 91.3333333333 Iteration: 400\n",
      "cost : 0.304157755019 Test Accuracy : 91.8 Iteration: 500\n",
      "cost : 0.277102404667 Test Accuracy : 91.9333333333 Iteration: 600\n",
      "cost : 0.256840370819 Test Accuracy : 92.4 Iteration: 700\n",
      "cost : 0.241965261011 Test Accuracy : 92.5333333333 Iteration: 800\n",
      "cost : 0.230382022404 Test Accuracy : 92.7333333333 Iteration: 900\n",
      "cost : 0.221024449305 Test Accuracy : 93.0 Iteration: 1000\n",
      "cost : 0.213893074444 Test Accuracy : 93.0 Iteration: 1100\n",
      "cost : 0.20841477015 Test Accuracy : 93.1333333333 Iteration: 1200\n",
      "cost : 0.203603890638 Test Accuracy : 93.2 Iteration: 1300\n",
      "cost : 0.199949141125 Test Accuracy : 93.1333333333 Iteration: 1400\n",
      "cost : 0.196773084385 Test Accuracy : 93.2666666667 Iteration: 1500\n",
      "cost : 0.194255862233 Test Accuracy : 93.3333333333 Iteration: 1600\n",
      "cost : 0.192026970336 Test Accuracy : 93.4666666667 Iteration: 1700\n",
      "cost : 0.190184297439 Test Accuracy : 93.4 Iteration: 1800\n",
      "cost : 0.188642937163 Test Accuracy : 93.4666666667 Iteration: 1900\n",
      "cost : 0.187140609089 Test Accuracy : 93.6 Iteration: 2000\n",
      "cost : 0.185998401952 Test Accuracy : 93.6 Iteration: 2100\n",
      "cost : 0.184871365175 Test Accuracy : 93.5333333333 Iteration: 2200\n",
      "cost : 0.183845478843 Test Accuracy : 93.6 Iteration: 2300\n",
      "cost : 0.182879892747 Test Accuracy : 93.6666666667 Iteration: 2400\n",
      "cost : 0.182014927159 Test Accuracy : 93.6666666667 Iteration: 2500\n",
      "cost : 0.181402968905 Test Accuracy : 93.6 Iteration: 2600\n",
      "cost : 0.18062620899 Test Accuracy : 93.6 Iteration: 2700\n",
      "cost : 0.179923888095 Test Accuracy : 93.6 Iteration: 2800\n",
      "cost : 0.179206283961 Test Accuracy : 93.6666666667 Iteration: 2900\n",
      "cost : 0.178600068147 Test Accuracy : 93.6666666667 Iteration: 3000\n",
      "cost : 0.177960361819 Test Accuracy : 93.6666666667 Iteration: 3100\n",
      "cost : 0.177387873132 Test Accuracy : 93.6666666667 Iteration: 3200\n",
      "cost : 0.176813328042 Test Accuracy : 93.6666666667 Iteration: 3300\n",
      "cost : 0.176363506221 Test Accuracy : 93.6666666667 Iteration: 3400\n",
      "cost : 0.175916947665 Test Accuracy : 93.8666666667 Iteration: 3500\n",
      "cost : 0.175415621418 Test Accuracy : 93.9333333333 Iteration: 3600\n",
      "cost : 0.174940084694 Test Accuracy : 93.9333333333 Iteration: 3700\n",
      "cost : 0.174497394011 Test Accuracy : 93.9333333333 Iteration: 3800\n",
      "cost : 0.174127146355 Test Accuracy : 93.9333333333 Iteration: 3900\n",
      "cost : 0.173769703217 Test Accuracy : 93.9333333333 Iteration: 4000\n",
      "cost : 0.173408269566 Test Accuracy : 93.9333333333 Iteration: 4100\n",
      "cost : 0.172981128575 Test Accuracy : 93.9333333333 Iteration: 4200\n",
      "cost : 0.172536416147 Test Accuracy : 94.0 Iteration: 4300\n",
      "cost : 0.172165685974 Test Accuracy : 94.1333333333 Iteration: 4400\n",
      "cost : 0.171894098321 Test Accuracy : 94.0 Iteration: 4500\n",
      "cost : 0.171667377429 Test Accuracy : 94.0 Iteration: 4600\n",
      "cost : 0.171358264966 Test Accuracy : 94.0 Iteration: 4700\n",
      "cost : 0.170978347699 Test Accuracy : 94.0666666667 Iteration: 4800\n",
      "cost : 0.170598104439 Test Accuracy : 94.0666666667 Iteration: 4900\n",
      "cost : 0.170289972625 Test Accuracy : 94.0666666667 Iteration: 5000\n",
      "cost : 0.170046916807 Test Accuracy : 94.1333333333 Iteration: 5100\n",
      "cost : 0.169786603712 Test Accuracy : 94.2 Iteration: 5200\n",
      "cost : 0.169545130198 Test Accuracy : 94.2 Iteration: 5300\n",
      "cost : 0.169294080517 Test Accuracy : 94.2 Iteration: 5400\n",
      "cost : 0.16909059468 Test Accuracy : 94.2 Iteration: 5500\n",
      "cost : 0.169008068386 Test Accuracy : 94.2 Iteration: 5600\n",
      "cost : 0.168861727851 Test Accuracy : 94.2 Iteration: 5700\n",
      "cost : 0.168665363452 Test Accuracy : 94.2666666667 Iteration: 5800\n",
      "cost : 0.168443127233 Test Accuracy : 94.2 Iteration: 5900\n",
      "cost : 0.168225423589 Test Accuracy : 94.2 Iteration: 6000\n",
      "cost : 0.168054320235 Test Accuracy : 94.2666666667 Iteration: 6100\n",
      "cost : 0.167810838911 Test Accuracy : 94.2666666667 Iteration: 6200\n",
      "cost : 0.167658241611 Test Accuracy : 94.2666666667 Iteration: 6300\n",
      "cost : 0.167452449812 Test Accuracy : 94.2666666667 Iteration: 6400\n",
      "cost : 0.16722439407 Test Accuracy : 94.3333333333 Iteration: 6500\n",
      "cost : 0.167051208254 Test Accuracy : 94.3333333333 Iteration: 6600\n",
      "cost : 0.166923606277 Test Accuracy : 94.2666666667 Iteration: 6700\n",
      "cost : 0.166863656745 Test Accuracy : 94.2666666667 Iteration: 6800\n",
      "cost : 0.166839144041 Test Accuracy : 94.1333333333 Iteration: 6900\n",
      "cost : 0.166578009278 Test Accuracy : 94.2666666667 Iteration: 7000\n",
      "cost : 0.166300438488 Test Accuracy : 94.3333333333 Iteration: 7100\n",
      "cost : 0.166166421622 Test Accuracy : 94.3333333333 Iteration: 7200\n",
      "cost : 0.166036097554 Test Accuracy : 94.2666666667 Iteration: 7300\n",
      "cost : 0.165956540626 Test Accuracy : 94.2666666667 Iteration: 7400\n",
      "cost : 0.165952682791 Test Accuracy : 94.2 Iteration: 7500\n",
      "cost : 0.165851257746 Test Accuracy : 94.2666666667 Iteration: 7600\n",
      "cost : 0.165708293603 Test Accuracy : 94.2666666667 Iteration: 7700\n",
      "cost : 0.165542030013 Test Accuracy : 94.2666666667 Iteration: 7800\n",
      "cost : 0.165316268095 Test Accuracy : 94.2666666667 Iteration: 7900\n",
      "Cost with Alpha 0.2 and lambda 10.0 is 0.16509995886 & Accuracy is 99.9428571429 %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a64b990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha is 0.2\n"
     ]
    }
   ],
   "source": [
    "# Get the best combination of weights and alpha\n",
    "cache, alph,scores = optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Test Data************************\n",
      "Test Data Optimized accuracy is 94.2 for alpha 0.2 and lambda 10.0\n",
      "*********************************************\n",
      "Maximum accuracy so far is 94.2\n",
      "Max alpha is 0.2\n",
      "Max lambda is 10.0\n"
     ]
    }
   ],
   "source": [
    "maxAcc = -sys.maxsize -1\n",
    "maxAlpha = None\n",
    "maxLambda = None\n",
    "for index, row in scores.iterrows():\n",
    "    a = row['alpha']\n",
    "    cache = row['cache']\n",
    "    lam = row['lamda']\n",
    "    cache['A0'] = X_test_mat\n",
    "    ykey = 'A' + str(hiddenLayers+1)\n",
    "    cache2 = forward_propagate(cache, hiddenLayers, activationFuncs)\n",
    "    Afinal = softmax(cache2['A'+str(hiddenLayers)])\n",
    "    acc = get_accuracy(y_test, Afinal)\n",
    "    if(acc > maxAcc):\n",
    "        maxAcc = acc\n",
    "        maxAlpha = a\n",
    "        maxLambda = lam\n",
    "    print('*****************Test Data************************')\n",
    "    print('Test Data Optimized accuracy is ' + str(acc) + ' for alpha ' + str(a) + ' and lambda ' + str(lam))\n",
    "print(\"*********************************************\")\n",
    "print(\"Maximum accuracy so far is \" + str(maxAcc))\n",
    "print(\"Max alpha is \" + str(maxAlpha))\n",
    "print(\"Max lambda is \" + str(maxLambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
