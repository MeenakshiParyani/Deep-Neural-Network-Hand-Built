{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import sys\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (30pts) Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot Encoding the categorical output values to binary by adding 1's for that index and 0's otherwise\n",
    "def oneHotEncode(y):\n",
    "    enc = pd.get_dummies(y['y'])\n",
    "    return np.matrix(enc)\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while forward propagation\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do squishing on the linear function\n",
    "def sigmoid(z):\n",
    "    return 1./(1+np.exp(-z))\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while backward propagation to get gradients\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do undo the squishing on the linear function\n",
    "def sigmoid_prime(z):\n",
    "    inv = (np.exp(-z))/(np.power((1+np.exp(-z)),2))\n",
    "    return inv\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z,0)\n",
    "    \n",
    "def relu_prime(z):\n",
    "    return np.where(z < 0, 0.0, 1.0)\n",
    "\n",
    "# Get the loss of for the training example\n",
    "def get_cost(Y, Yhat, lamda, Wl):\n",
    "    m= Y.shape[1]\n",
    "    loss = np.multiply(np.log(Yhat),Y) + np.multiply((1.-Y), np.log(1. - Yhat))\n",
    "    loss = np.sum(loss)\n",
    "    cost = -1./m * np.sum(loss) + ((lamda/(2*m)) * np.sum(np.square(Wl)))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost\n",
    "\n",
    "# Forward propagation to calculate yHat by applying activation function twice\n",
    "def forward_propagate(cache, layerCount, activationFuncs):\n",
    "    for l in range(1, layerCount+1):\n",
    "        Zl =  'Z' + str(l)\n",
    "        Al =  'A' + str(l)\n",
    "        Wl =  'W' + str(l)\n",
    "        Al1 = 'A' + str(l-1)\n",
    "        bl =  'b' + str(l)\n",
    "        cache[Zl] = np.dot(cache[Wl], cache[Al1]) + cache[bl]\n",
    "        activationFunc = activationFuncs[l] + '(cache[Zl])'\n",
    "        cache[Al] = eval(activationFunc)\n",
    "    return cache\n",
    "\n",
    "# Backward Propagation function to calculate the gradients\n",
    "def back_propagate(cache, layerCount, deactivationFuncs):\n",
    "    gradients = {}\n",
    "    m = cache['A0'].shape[1]\n",
    "    for l in xrange(layerCount, 0, -1):\n",
    "        if(l==layerCount): # is last layer\n",
    "            dZl = cache['A' + str(l)] - cache['A' + str(l+1)]    \n",
    "        else:\n",
    "            Zl = 'Z'+ str(l)\n",
    "            dZl1 = 'dZ'+str(l+1)\n",
    "            deacFunc = deactivationFuncs[l] + '(cache[Zl])'\n",
    "            actv_prime = eval(deacFunc)\n",
    "            term1 = cache['W'+str(l+1)].T\n",
    "            term2 = gradients[dZl1]\n",
    "            product = np.dot( term1, term2)\n",
    "            dZl = np.multiply(product, actv_prime) \n",
    "        gradients['dZ' + str(l)] = dZl\n",
    "        Al1 = cache['A' + str(l-1)]\n",
    "#         print('--dZ'+str(l))\n",
    "#         print(dZl.shape)\n",
    "#         print('A'+str(l-1))\n",
    "#         print( Al1.shape)\n",
    "        dWl = (1./m) * np.dot(dZl, Al1.T)\n",
    "        dbl = (1./m) * np.sum(dZl, axis=1)\n",
    "#         print('dW' +str(l))\n",
    "#         print(dWl.shape)\n",
    "#         print('db' +str(l))\n",
    "#         print(dbl.shape)\n",
    "        gradients['dW' + str(l)] = dWl\n",
    "        gradients['db' + str(l)] = dbl\n",
    "    return gradients  \n",
    "\n",
    "\n",
    "def gradientDescent(X, Y, YOrg, alpha, iters, hiddenLayers, layerSizes, activationFuncs, deactivationFuncs, lamda):  \n",
    "    \n",
    "    cache = initialize_parameters(hiddenLayers, layerSizes,activationFuncs )\n",
    "    cache['A0'] = X\n",
    "    ykey = 'A' + str(hiddenLayers+1)\n",
    "    cache[ykey] = Y\n",
    "    old_cost = sys.maxsize\n",
    "    new_cost = sys.maxsize\n",
    "    cost_history = []\n",
    "    for i in range(iters):\n",
    "        # Call Forward propagation to calculate yHat\n",
    "        cache = forward_propagate(cache, hiddenLayers, activationFuncs)\n",
    "        old_cost = new_cost\n",
    "        Afinal = cache['A'+str(hiddenLayers)]\n",
    "        new_cost = get_cost(Y, cache['A'+str(hiddenLayers)], lamda, cache['W'+str(hiddenLayers)]) # Regularization\n",
    "        gradients = back_propagate(cache, hiddenLayers, deactivationFuncs)\n",
    "        for l in range(1,hiddenLayers+1):\n",
    "            Wl = cache['W'+str(l)]\n",
    "            gradients['dW'+str(l)] = gradients['dW'+str(l)] + (lamda/m) * Wl  # Regularization\n",
    "            Wl = Wl - alpha * gradients['dW'+str(l)]\n",
    "            cache['W'+str(l)] = Wl\n",
    "            bl = cache['b'+str(l)]\n",
    "            bl = bl - alpha * gradients['db'+str(l)]\n",
    "            cache['b'+str(l)] = bl\n",
    "        if(abs(old_cost - new_cost) < 0.00000001):\n",
    "            print(\"breaking\" + str(old_cost) + str(new_cost))\n",
    "            break;\n",
    "        if(i%100 ==0):\n",
    "            print (\"cost : \" + str(new_cost) + \" Old cost : \" + str(old_cost) + \" Iteration: \" + str(i))\n",
    "        cost_history.append(new_cost)\n",
    "    Afinal = softmax(Afinal) # Apply softmax to get the actual labels\n",
    "    cache['A'+str(hiddenLayers)] = Afinal\n",
    "    accuracy = get_accuracy(YOrg, Afinal)\n",
    "    return cache, cost_history, new_cost, accuracy\n",
    "\n",
    "# Softmax activation function to get the probablity of the classes\n",
    "def softmax(z):\n",
    "    softMax = (np.exp(z) / np.sum(np.exp(z),axis=0))\n",
    "    softMax = np.matrix(np.argmax(softMax,axis=0)).T\n",
    "    return softMax\n",
    "\n",
    "def plotCostHistory(cost_history, alpha, i):\n",
    "     line = plt.plot(cost_history, label=alpha)\n",
    "     plt.ylabel('Cost');\n",
    "     plt.ylim( (0, 4) )\n",
    "     plt.xlabel('Iterations');\n",
    "     plt.title('Cost Progression with Iterations for different learning rates')\n",
    "     plt.legend()\n",
    "        \n",
    "def get_accuracy(Y, Ypred):\n",
    "    Y = np.matrix(Y)\n",
    "    numcorrect = 0\n",
    "    for (x,y) in zip(Ypred,Y):\n",
    "        if(x[0]==y[0]):\n",
    "            numcorrect+=1\n",
    "    accuracy=numcorrect*100.0/len(Y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the training data\n",
    "data_train = pd.read_csv('exam1_train.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "# Remove the un-necessary column\n",
    "df_train = data_train.drop('Unnamed: 0',axis=1)\n",
    "m = df_train.shape[0]\n",
    "\n",
    "y_train = pd.DataFrame(df_train['y'])\n",
    "X_train = df_train.drop(['y'], axis=1)\n",
    "\n",
    "X_train_mat = np.matrix(X_train).T\n",
    "y_train_mat = oneHotEncode(y_train).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y\n",
      "202  4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d1b2950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the test data\n",
    "data_test = pd.read_csv('exam1_test.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "# Remove the un-necessary column\n",
    "df_test = data_test.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "y_test = pd.DataFrame(df_test['y'])\n",
    "X_test = df_test.drop(['y'], axis=1)\n",
    "\n",
    "X_test_mat = np.matrix(X_test).T\n",
    "y_test_mat = oneHotEncode(y_test).T\n",
    "\n",
    "# Plot the selected pixel\n",
    "num = 202\n",
    "pixels = np.array(X_test[num:num+1], dtype='uint8')\n",
    "print(y_test[num:num+1])\n",
    "pixels = pixels.reshape((20, 20)).T\n",
    "plt.imshow(pixels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (10pts) Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize weights // TODO - change the initialization method\n",
    "\n",
    "np.random.seed(1) # Setting random seed to 1\n",
    "def initialize_parameters(hiddenLayers, layerSizes, activationFuncs):\n",
    "    cache = {}\n",
    "    for l in range(1,hiddenLayers+1):\n",
    "        #print(l)\n",
    "        Wl = 'W' + str(l)\n",
    "        bl = 'b' + str(l)\n",
    "        #print(layerSizes[l])\n",
    "        #print(layerSizes[l-1])\n",
    "        print('activation is ' + str(activationFuncs[l]) + ' for layer ' + str(l))\n",
    "        if(activationFuncs[l]=='sigmoid'):\n",
    "            print('activation is sigmoid for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * np.sqrt(2./layerSizes[l-1])\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        elif(activationFuncs[l]=='relu'):\n",
    "            print('activation is relu for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * (2./np.sqrt(layerSizes[l-1]))\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        else:\n",
    "            print('activation is random for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * 0.01 ## Random\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        l=l+1\n",
    "    #print(cache)\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (30pts) Deep Neural Network model with more than 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of hidden layers including output layer are - 3\n"
     ]
    }
   ],
   "source": [
    "# Defining Hyperparameters\n",
    "hiddenLayers = 3\n",
    "layerSizes = [400, 100, 40, 10] # As specified in assignment requirements, can have as many hidden layers\n",
    "print(\" Number of hidden layers including output layer are - \" + str(hiddenLayers))\n",
    "activationFuncs = ['', 'relu', 'relu', 'sigmoid']\n",
    "deactivationFuncs = ['', 'relu_prime', 'relu_prime', 'sigmoid_prime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cache = initialize_parameters(hiddenLayers, layerSizes, activationFuncs)\n",
    "# print(cache['W1'].shape)\n",
    "# # print(cache['W2'].shape)\n",
    "# # print(cache['W3'].shape)\n",
    "# # print(cache['b1'].shape)\n",
    "# # print(cache['b2'].shape)\n",
    "# # print(cache['b3'].shape)\n",
    "# cache['A0'] = X_train_mat\n",
    "# ykey = 'A' + str(len(layerSizes))\n",
    "# cache[ykey] = y_train_mat\n",
    "# cache = forward_propagate(cache, hiddenLayers, ['', 'apply_sigmoid', 'apply_sigmoid','apply_sigmoid'])\n",
    "# # print(cache['A1'])\n",
    "# # print(cache['A2'])\n",
    "# # print(cache['A3'])\n",
    "# # print(cache['Z2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# gradients = back_propagate(cache, hiddenLayers, deactivationFuncs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. (10pts) Predictions\n",
    "\n",
    "### 6. (20pts) Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimization function to check cost propagartion for different learning rates\n",
    "def optimize():\n",
    "    lamdas = [10.0]\n",
    "    alpha = [0.2]\n",
    "    i=0\n",
    "    scores = pd.DataFrame(columns=['alpha','cache','accuracy', 'lamda'])\n",
    "    print('*****************Training Data*********************')\n",
    "    for l in lamdas:\n",
    "        for a in alpha:\n",
    "            cache, cost_history, new_cost, acc = gradientDescent(X_train_mat, y_train_mat, y_train, a, 5210, \n",
    "                                                hiddenLayers, layerSizes, activationFuncs, deactivationFuncs, l)\n",
    "            scores.loc[i] = pd.Series({'alpha':a, 'cache': cache, 'accuracy':acc, 'lamda':l})\n",
    "            print(\"Cost with \" + \"Alpha \" + str(a) + \" and lambda \" + str(l) + \" is \" + str(new_cost) + \" & \" + \"Accuracy is \" + str(acc) + \" %\")\n",
    "            plotCostHistory(cost_history, a, i)\n",
    "            i+=1\n",
    "        least_cost_comb = scores['accuracy'].idxmax()\n",
    "        alph = scores.iloc[[least_cost_comb]]['alpha'][least_cost_comb]\n",
    "        cache = scores.iloc[[least_cost_comb]]['cache'][least_cost_comb]\n",
    "        plt.show()\n",
    "        print('Best alpha is ' + str(alph))\n",
    "    return cache, alph,scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Training Data*********************\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 2\n",
      "activation is sigmoid for layer 3\n",
      "activation is sigmoid for layer 3\n",
      "cost : 8.31698315296 Old cost : 9223372036854775807 Iteration: 0\n",
      "cost : 0.674380883014 Old cost : 0.676863759342 Iteration: 100\n",
      "cost : 0.479245154929 Old cost : 0.480374497937 Iteration: 200\n",
      "cost : 0.394215509003 Old cost : 0.394870238477 Iteration: 300\n",
      "cost : 0.341360980847 Old cost : 0.341793600208 Iteration: 400\n",
      "cost : 0.304157755019 Old cost : 0.304436868246 Iteration: 500\n",
      "cost : 0.277102404667 Old cost : 0.277317879773 Iteration: 600\n",
      "cost : 0.256840370819 Old cost : 0.257027491487 Iteration: 700\n",
      "cost : 0.241965261011 Old cost : 0.242080430246 Iteration: 800\n",
      "cost : 0.230382022404 Old cost : 0.230462205594 Iteration: 900\n",
      "cost : 0.221024449305 Old cost : 0.221132672699 Iteration: 1000\n",
      "cost : 0.213893074444 Old cost : 0.213958553109 Iteration: 1100\n",
      "cost : 0.20841477015 Old cost : 0.208470515794 Iteration: 1200\n",
      "cost : 0.203603890638 Old cost : 0.20361498456 Iteration: 1300\n",
      "cost : 0.199949141125 Old cost : 0.199974636362 Iteration: 1400\n",
      "cost : 0.196773084385 Old cost : 0.196790297016 Iteration: 1500\n",
      "cost : 0.194255862233 Old cost : 0.194268883414 Iteration: 1600\n",
      "cost : 0.192026970336 Old cost : 0.192053996054 Iteration: 1700\n",
      "cost : 0.190184297439 Old cost : 0.190191701522 Iteration: 1800\n",
      "cost : 0.188642937163 Old cost : 0.188663968284 Iteration: 1900\n",
      "cost : 0.187140609089 Old cost : 0.187134854412 Iteration: 2000\n",
      "cost : 0.185998401952 Old cost : 0.186007857652 Iteration: 2100\n",
      "cost : 0.184871365175 Old cost : 0.184886088677 Iteration: 2200\n",
      "cost : 0.183845478843 Old cost : 0.183859971064 Iteration: 2300\n",
      "cost : 0.182879892747 Old cost : 0.182885561295 Iteration: 2400\n",
      "cost : 0.182014927159 Old cost : 0.182023391787 Iteration: 2500\n",
      "cost : 0.181402968905 Old cost : 0.1814193848 Iteration: 2600\n",
      "cost : 0.18062620899 Old cost : 0.18066638542 Iteration: 2700\n",
      "cost : 0.179923888095 Old cost : 0.179950937331 Iteration: 2800\n",
      "cost : 0.179206283961 Old cost : 0.179222378205 Iteration: 2900\n",
      "cost : 0.178600068147 Old cost : 0.178620627794 Iteration: 3000\n",
      "cost : 0.177960361819 Old cost : 0.177983009531 Iteration: 3100\n",
      "cost : 0.177387873132 Old cost : 0.177409327815 Iteration: 3200\n",
      "cost : 0.176813328042 Old cost : 0.176827879666 Iteration: 3300\n",
      "cost : 0.176363506221 Old cost : 0.17633935625 Iteration: 3400\n",
      "cost : 0.175916947665 Old cost : 0.1759380157 Iteration: 3500\n",
      "cost : 0.175415621418 Old cost : 0.175443581374 Iteration: 3600\n",
      "cost : 0.174940084694 Old cost : 0.174962099677 Iteration: 3700\n",
      "cost : 0.174497394011 Old cost : 0.174523325852 Iteration: 3800\n",
      "cost : 0.174127146355 Old cost : 0.174140945001 Iteration: 3900\n",
      "cost : 0.173769703217 Old cost : 0.173787904995 Iteration: 4000\n",
      "cost : 0.173408269566 Old cost : 0.173432976786 Iteration: 4100\n",
      "cost : 0.172981128575 Old cost : 0.172986405829 Iteration: 4200\n",
      "cost : 0.172536416147 Old cost : 0.172558387648 Iteration: 4300\n",
      "cost : 0.172165685974 Old cost : 0.172175650325 Iteration: 4400\n",
      "cost : 0.171894098321 Old cost : 0.171884309015 Iteration: 4500\n",
      "cost : 0.171667377429 Old cost : 0.171651245539 Iteration: 4600\n",
      "cost : 0.171358264966 Old cost : 0.171356575243 Iteration: 4700\n",
      "cost : 0.170978347699 Old cost : 0.171000614281 Iteration: 4800\n",
      "cost : 0.170598104439 Old cost : 0.17062110628 Iteration: 4900\n",
      "cost : 0.170289972625 Old cost : 0.17029737801 Iteration: 5000\n",
      "cost : 0.170046916807 Old cost : 0.170038739024 Iteration: 5100\n",
      "cost : 0.169786603712 Old cost : 0.169780970408 Iteration: 5200\n",
      "Cost with Alpha 0.2 and lambda 10.0 is 0.169914019588 & Accuracy is 99.8857142857 %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nedPTdOr0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111d88d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha is 0.2\n"
     ]
    }
   ],
   "source": [
    "# Get the best combination of weights and alpha\n",
    "cache, alph,scores = optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Test Data************************\n",
      "Test Data Optimized accuracy is 94.2666666667 for alpha 0.2 and lambda 10.0\n",
      "*********************************************\n",
      "Maximum accuracy so far is 94.2666666667\n",
      "Max alpha is 0.2\n",
      "Max lambda is 10.0\n"
     ]
    }
   ],
   "source": [
    "maxAcc = -sys.maxsize -1\n",
    "maxAlpha = None\n",
    "maxLambda = None\n",
    "for index, row in scores.iterrows():\n",
    "    a = row['alpha']\n",
    "    cache = row['cache']\n",
    "    lam = row['lamda']\n",
    "    cache['A0'] = X_test_mat\n",
    "    ykey = 'A' + str(hiddenLayers+1)\n",
    "    cache2 = forward_propagate(cache, hiddenLayers, activationFuncs)\n",
    "    Afinal = softmax(cache2['A'+str(hiddenLayers)])\n",
    "    acc = get_accuracy(y_test, Afinal)\n",
    "    if(acc > maxAcc):\n",
    "        maxAcc = acc\n",
    "        maxAlpha = a\n",
    "        maxLambda = lam\n",
    "    print('*****************Test Data************************')\n",
    "    print('Test Data Optimized accuracy is ' + str(acc) + ' for alpha ' + str(a) + ' and lambda ' + str(lam))\n",
    "print(\"*********************************************\")\n",
    "print(\"Maximum accuracy so far is \" + str(maxAcc))\n",
    "print(\"Max alpha is \" + str(maxAlpha))\n",
    "print(\"Max lambda is \" + str(maxLambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
