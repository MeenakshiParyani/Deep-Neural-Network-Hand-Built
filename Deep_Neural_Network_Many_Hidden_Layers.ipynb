{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import sys\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (30pts) Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot Encoding the categorical output values to binary by adding 1's for that index and 0's otherwise\n",
    "def oneHotEncode(y):\n",
    "    enc = pd.get_dummies(y['y'])\n",
    "    return np.matrix(enc)\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while forward propagation\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do squishing on the linear function\n",
    "def sigmoid(z):\n",
    "    return 1./(1+np.exp(-z))\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while backward propagation to get gradients\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do undo the squishing on the linear function\n",
    "def sigmoid_prime(z):\n",
    "    inv = (np.exp(-z))/(np.power((1+np.exp(-z)),2))\n",
    "    return inv\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z,0)\n",
    "    \n",
    "def relu_prime(z):\n",
    "    return np.where(z < 0, 0.0, 1.0)\n",
    "\n",
    "# Get the loss of for the training example\n",
    "def get_cost(Y, Yhat, lamda, Wl):\n",
    "    m= Y.shape[1]\n",
    "    loss = np.multiply(np.log(Yhat),Y) + np.multiply((1.-Y), np.log(1. - Yhat))\n",
    "    loss = np.sum(loss)\n",
    "    cost = -1./m * np.sum(loss) + ((lamda/(2*m)) * np.sum(np.square(Wl)))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost\n",
    "\n",
    "# Forward propagation to calculate yHat by applying activation function twice\n",
    "def forward_propagate(cache, layerCount, activationFuncs):\n",
    "    for l in range(1, layerCount+1):\n",
    "        Zl =  'Z' + str(l)\n",
    "        Al =  'A' + str(l)\n",
    "        Wl =  'W' + str(l)\n",
    "        Al1 = 'A' + str(l-1)\n",
    "        bl =  'b' + str(l)\n",
    "        cache[Zl] = np.dot(cache[Wl], cache[Al1]) + cache[bl]\n",
    "        activationFunc = activationFuncs[l] + '(cache[Zl])'\n",
    "        cache[Al] = eval(activationFunc)\n",
    "    return cache\n",
    "\n",
    "# Backward Propagation function to calculate the gradients\n",
    "def back_propagate(cache, layerCount, deactivationFuncs):\n",
    "    gradients = {}\n",
    "    m = cache['A0'].shape[1]\n",
    "    for l in xrange(layerCount, 0, -1):\n",
    "        if(l==layerCount): # is last layer\n",
    "            dZl = cache['A' + str(l)] - cache['A' + str(l+1)]    \n",
    "        else:\n",
    "            Zl = 'Z'+ str(l)\n",
    "            dZl1 = 'dZ'+str(l+1)\n",
    "            deacFunc = deactivationFuncs[l] + '(cache[Zl])'\n",
    "            actv_prime = eval(deacFunc)\n",
    "            term1 = cache['W'+str(l+1)].T\n",
    "            term2 = gradients[dZl1]\n",
    "            product = np.dot( term1, term2)\n",
    "            dZl = np.multiply(product, actv_prime) \n",
    "        gradients['dZ' + str(l)] = dZl\n",
    "        Al1 = cache['A' + str(l-1)]\n",
    "#         print('--dZ'+str(l))\n",
    "#         print(dZl.shape)\n",
    "#         print('A'+str(l-1))\n",
    "#         print( Al1.shape)\n",
    "        dWl = (1./m) * np.dot(dZl, Al1.T)\n",
    "        dbl = (1./m) * np.sum(dZl, axis=1)\n",
    "#         print('dW' +str(l))\n",
    "#         print(dWl.shape)\n",
    "#         print('db' +str(l))\n",
    "#         print(dbl.shape)\n",
    "        gradients['dW' + str(l)] = dWl\n",
    "        gradients['db' + str(l)] = dbl\n",
    "    return gradients  \n",
    "\n",
    "\n",
    "def gradientDescent(X, Y, YOrg, alpha, iters, hiddenLayers, layerSizes, activationFuncs, deactivationFuncs, lamda):  \n",
    "    \n",
    "    cache = initialize_parameters(hiddenLayers, layerSizes,activationFuncs )\n",
    "    cache['A0'] = X\n",
    "    ykey = 'A' + str(hiddenLayers+1)\n",
    "    cache[ykey] = Y\n",
    "    old_cost = sys.maxsize\n",
    "    new_cost = sys.maxsize\n",
    "    cost_history = []\n",
    "    for i in range(iters):\n",
    "        # Call Forward propagation to calculate yHat\n",
    "        cache = forward_propagate(cache, hiddenLayers, activationFuncs)\n",
    "        old_cost = new_cost\n",
    "        Afinal = cache['A'+str(hiddenLayers)]\n",
    "        new_cost = get_cost(Y, cache['A'+str(hiddenLayers)], lamda, cache['W'+str(hiddenLayers)]) # Regularization\n",
    "        gradients = back_propagate(cache, hiddenLayers, deactivationFuncs)\n",
    "        for l in range(1,hiddenLayers+1):\n",
    "            Wl = cache['W'+str(l)]\n",
    "            gradients['dW'+str(l)] = gradients['dW'+str(l)] + (lamda/m) * Wl  # Regularization\n",
    "            Wl = Wl - alpha * gradients['dW'+str(l)]\n",
    "            cache['W'+str(l)] = Wl\n",
    "            bl = cache['b'+str(l)]\n",
    "            bl = bl - alpha * gradients['db'+str(l)]\n",
    "            cache['b'+str(l)] = bl\n",
    "        if(abs(old_cost - new_cost) < 0.00000001):\n",
    "            print(\"breaking\" + str(old_cost) + str(new_cost))\n",
    "            break;\n",
    "        if(i%100 ==0):\n",
    "            print (\"cost : \" + str(new_cost) + \" Old cost : \" + str(old_cost) + \" Iteration: \" + str(i))\n",
    "        cost_history.append(new_cost)\n",
    "    Afinal = softmax(Afinal) # Apply softmax to get the actual labels\n",
    "    cache['A'+str(hiddenLayers)] = Afinal\n",
    "    accuracy = get_accuracy(YOrg, Afinal)\n",
    "    return cache, cost_history, new_cost, accuracy\n",
    "\n",
    "# Softmax activation function to get the probablity of the classes\n",
    "def softmax(z):\n",
    "    softMax = (np.exp(z) / np.sum(np.exp(z),axis=0))\n",
    "    softMax = np.matrix(np.argmax(softMax,axis=0)).T\n",
    "    return softMax\n",
    "\n",
    "def plotCostHistory(cost_history, alpha, i):\n",
    "     line = plt.plot(cost_history, label=alpha)\n",
    "     plt.ylabel('Cost');\n",
    "     plt.ylim( (0, 4) )\n",
    "     plt.xlabel('Iterations');\n",
    "     plt.title('Cost Progression with Iterations for different learning rates')\n",
    "     plt.legend()\n",
    "        \n",
    "def get_accuracy(Y, Ypred):\n",
    "    Y = np.matrix(Y)\n",
    "    numcorrect = 0\n",
    "    for (x,y) in zip(Ypred,Y):\n",
    "        if(x[0]==y[0]):\n",
    "            numcorrect+=1\n",
    "    accuracy=numcorrect*100.0/len(Y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the training data\n",
    "data_train = pd.read_csv('exam1_train.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "# Remove the un-necessary column\n",
    "df_train = data_train.drop('Unnamed: 0',axis=1)\n",
    "m = df_train.shape[0]\n",
    "\n",
    "y_train = pd.DataFrame(df_train['y'])\n",
    "X_train = df_train.drop(['y'], axis=1)\n",
    "\n",
    "X_train_mat = np.matrix(X_train).T\n",
    "y_train_mat = oneHotEncode(y_train).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y\n",
      "202  4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b6cf950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the test data\n",
    "data_test = pd.read_csv('exam1_test.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "# Remove the un-necessary column\n",
    "df_test = data_test.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "y_test = pd.DataFrame(df_test['y'])\n",
    "X_test = df_test.drop(['y'], axis=1)\n",
    "\n",
    "X_test_mat = np.matrix(X_test).T\n",
    "y_test_mat = oneHotEncode(y_test).T\n",
    "\n",
    "# Plot the selected pixel\n",
    "num = 202\n",
    "pixels = np.array(X_test[num:num+1], dtype='uint8')\n",
    "print(y_test[num:num+1])\n",
    "pixels = pixels.reshape((20, 20)).T\n",
    "plt.imshow(pixels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (10pts) Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize weights // TODO - change the initialization method\n",
    "\n",
    "np.random.seed(1) # Setting random seed to 1\n",
    "def initialize_parameters(hiddenLayers, layerSizes, activationFuncs):\n",
    "    cache = {}\n",
    "    for l in range(1,hiddenLayers+1):\n",
    "        #print(l)\n",
    "        Wl = 'W' + str(l)\n",
    "        bl = 'b' + str(l)\n",
    "        #print(layerSizes[l])\n",
    "        #print(layerSizes[l-1])\n",
    "        print('activation is ' + str(activationFuncs[l]) + ' for layer ' + str(l))\n",
    "        if(activationFuncs[l]=='sigmoid'):\n",
    "            print('activation is sigmoid for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * np.sqrt(2./layerSizes[l-1])\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        elif(activationFuncs[l]=='relu'):\n",
    "            print('activation is relu for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * (2./np.sqrt(layerSizes[l-1]))\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        else:\n",
    "            print('activation is random for layer ' + str(l))\n",
    "            cache[Wl] =  np.random.randn(layerSizes[l], layerSizes[l-1]) * 0.01 ## Random\n",
    "            cache[bl] =  np.zeros((layerSizes[l],1))\n",
    "        l=l+1\n",
    "    #print(cache)\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (30pts) Deep Neural Network model with more than 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of hidden layers including output layer are - 3\n"
     ]
    }
   ],
   "source": [
    "# Defining Hyperparameters\n",
    "hiddenLayers = 3\n",
    "layerSizes = [400, 100, 40, 10] # As specified in assignment requirements, can have as many hidden layers\n",
    "print(\" Number of hidden layers including output layer are - \" + str(hiddenLayers))\n",
    "activationFuncs = ['', 'relu', 'relu', 'sigmoid']\n",
    "deactivationFuncs = ['', 'relu_prime', 'relu_prime', 'sigmoid_prime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cache = initialize_parameters(hiddenLayers, layerSizes, activationFuncs)\n",
    "# print(cache['W1'].shape)\n",
    "# # print(cache['W2'].shape)\n",
    "# # print(cache['W3'].shape)\n",
    "# # print(cache['b1'].shape)\n",
    "# # print(cache['b2'].shape)\n",
    "# # print(cache['b3'].shape)\n",
    "# cache['A0'] = X_train_mat\n",
    "# ykey = 'A' + str(len(layerSizes))\n",
    "# cache[ykey] = y_train_mat\n",
    "# cache = forward_propagate(cache, hiddenLayers, ['', 'apply_sigmoid', 'apply_sigmoid','apply_sigmoid'])\n",
    "# # print(cache['A1'])\n",
    "# # print(cache['A2'])\n",
    "# # print(cache['A3'])\n",
    "# # print(cache['Z2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# gradients = back_propagate(cache, hiddenLayers, deactivationFuncs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. (10pts) Predictions\n",
    "\n",
    "### 6. (20pts) Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimization function to check cost propagartion for different learning rates\n",
    "def optimize():\n",
    "    lamdas = [14.0]\n",
    "    alpha = [0.2]\n",
    "    i=0\n",
    "    scores = pd.DataFrame(columns=['alpha','cache','accuracy', 'lamda'])\n",
    "    print('*****************Training Data*********************')\n",
    "    for l in lamdas:\n",
    "        for a in alpha:\n",
    "            cache, cost_history, new_cost, acc = gradientDescent(X_train_mat, y_train_mat, y_train, a, 5000, \n",
    "                                                hiddenLayers, layerSizes, activationFuncs, deactivationFuncs, l)\n",
    "            scores.loc[i] = pd.Series({'alpha':a, 'cache': cache, 'accuracy':acc, 'lamda':l})\n",
    "            print(\"Cost with \" + \"Alpha \" + str(a) + \" and lambda \" + str(l) + \" is \" + str(new_cost) + \" & \" + \"Accuracy is \" + str(acc) + \" %\")\n",
    "            plotCostHistory(cost_history, a, i)\n",
    "            i+=1\n",
    "        least_cost_comb = scores['accuracy'].idxmax()\n",
    "        alph = scores.iloc[[least_cost_comb]]['alpha'][least_cost_comb]\n",
    "        cache = scores.iloc[[least_cost_comb]]['cache'][least_cost_comb]\n",
    "        plt.show()\n",
    "        print('Best alpha is ' + str(alph))\n",
    "    return cache, alph,scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Training Data*********************\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 1\n",
      "activation is relu for layer 2\n",
      "activation is relu for layer 2\n",
      "activation is sigmoid for layer 3\n",
      "activation is sigmoid for layer 3\n",
      "cost : 8.32843109401 Old cost : 9223372036854775807 Iteration: 0\n",
      "cost : 0.70372160838 Old cost : 0.705579085448 Iteration: 100\n",
      "cost : 0.51424452361 Old cost : 0.5152986639 Iteration: 200\n",
      "cost : 0.43547576885 Old cost : 0.436056968557 Iteration: 300\n",
      "cost : 0.387463458248 Old cost : 0.387829137903 Iteration: 400\n",
      "cost : 0.353801086285 Old cost : 0.354077053816 Iteration: 500\n",
      "cost : 0.330814579736 Old cost : 0.330964199479 Iteration: 600\n",
      "cost : 0.31269257887 Old cost : 0.312832121946 Iteration: 700\n",
      "cost : 0.299547235885 Old cost : 0.299668976941 Iteration: 800\n",
      "cost : 0.289051252739 Old cost : 0.289186500114 Iteration: 900\n",
      "cost : 0.28072449609 Old cost : 0.280817748007 Iteration: 1000\n",
      "cost : 0.274318937102 Old cost : 0.274427870103 Iteration: 1100\n",
      "cost : 0.268306823272 Old cost : 0.268330702364 Iteration: 1200\n",
      "cost : 0.263765520161 Old cost : 0.26384441426 Iteration: 1300\n",
      "cost : 0.259900939257 Old cost : 0.259964030794 Iteration: 1400\n",
      "cost : 0.256794249839 Old cost : 0.256804953727 Iteration: 1500\n",
      "cost : 0.254021765968 Old cost : 0.254037179814 Iteration: 1600\n",
      "cost : 0.250826635659 Old cost : 0.250846030419 Iteration: 1700\n",
      "cost : 0.24894052017 Old cost : 0.248979270975 Iteration: 1800\n",
      "cost : 0.247383464822 Old cost : 0.247360310206 Iteration: 1900\n",
      "cost : 0.24512369004 Old cost : 0.245152793316 Iteration: 2000\n",
      "cost : 0.244375868375 Old cost : 0.244421201388 Iteration: 2100\n",
      "cost : 0.242904537341 Old cost : 0.242831568944 Iteration: 2200\n",
      "cost : 0.241056236045 Old cost : 0.241052550065 Iteration: 2300\n",
      "cost : 0.240388780435 Old cost : 0.240440513827 Iteration: 2400\n",
      "cost : 0.239704970833 Old cost : 0.239770906622 Iteration: 2500\n"
     ]
    }
   ],
   "source": [
    "# Get the best combination of weights and alpha\n",
    "cache, alph,scores = optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maxAcc = -sys.maxsize -1\n",
    "maxAlpha = None\n",
    "maxLambda = None\n",
    "for index, row in scores.iterrows():\n",
    "    a = row['alpha']\n",
    "    cache = row['cache']\n",
    "    lam = row['lamda']\n",
    "    cache['A0'] = X_test_mat\n",
    "    ykey = 'A' + str(hiddenLayers+1)\n",
    "    cache2 = forward_propagate(cache, hiddenLayers, activationFuncs)\n",
    "    Afinal = softmax(cache2['A'+str(hiddenLayers)])\n",
    "    acc = get_accuracy(y_test, Afinal)\n",
    "    if(acc > maxAcc):\n",
    "        maxAcc = acc\n",
    "        maxAlpha = a\n",
    "        maxLambda = lam\n",
    "    print('*****************Test Data************************')\n",
    "    print('Test Data Optimized accuracy is ' + str(acc) + ' for alpha ' + str(a) + ' and lambda ' + str(lam))\n",
    "print(\"*********************************************\")\n",
    "print(\"Maximum accuracy so far is \" + str(maxAcc))\n",
    "print(\"Max alpha is \" + str(maxAlpha))\n",
    "print(\"Max lambda is \" + str(maxLambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
